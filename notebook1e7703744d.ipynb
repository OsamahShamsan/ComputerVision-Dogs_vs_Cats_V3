{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:29.059138Z","iopub.execute_input":"2025-12-06T20:25:29.059840Z","iopub.status.idle":"2025-12-06T20:25:29.076044Z","shell.execute_reply.started":"2025-12-06T20:25:29.059808Z","shell.execute_reply":"2025-12-06T20:25:29.075234Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dogs-vs-cats/test1.zip\n/kaggle/input/dogs-vs-cats/train.zip\n/kaggle/input/dogs-vs-cats/sampleSubmission.csv\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"/kaggle/input/dogs-vs-cats/test1.zip\n/kaggle/input/dogs-vs-cats/train.zip\n/kaggle/input/dogs-vs-cats/sampleSubmission.csv","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.cuda.device_count())\nfor i in range(torch.cuda.device_count()):\n    print(i, torch.cuda.get_device_name(i))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:29.077198Z","iopub.execute_input":"2025-12-06T20:25:29.077528Z","iopub.status.idle":"2025-12-06T20:25:29.084897Z","shell.execute_reply.started":"2025-12-06T20:25:29.077509Z","shell.execute_reply":"2025-12-06T20:25:29.084117Z"}},"outputs":[{"name":"stdout","text":"1\n0 Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"1\n0 Tesla P100-PCIE-16GB","metadata":{}},{"cell_type":"code","source":"import os, zipfile, random, shutil, glob\nfrom pathlib import Path\n\nrandom.seed(42)\n\n# Paths\ntrain_zip = \"/kaggle/input/dogs-vs-cats/train.zip\"\ntest_zip  = \"/kaggle/input/dogs-vs-cats/test1.zip\"\nwork_dir  = \"/kaggle/working/dogs_cats\"\n\nraw_train = f\"{work_dir}/raw_train\"\nraw_test  = f\"{work_dir}/raw_test\"\n\nos.makedirs(raw_train, exist_ok=True)\nos.makedirs(raw_test, exist_ok=True)\n\n# Unzip\nwith zipfile.ZipFile(train_zip, 'r') as z:\n    z.extractall(raw_train)\n\nwith zipfile.ZipFile(test_zip, 'r') as z:\n    z.extractall(raw_test)\n\n# The Kaggle dataset usually extracts to raw_train/train/*.jpg\ntrain_imgs = sorted(glob.glob(f\"{raw_train}/train/*.jpg\"))\n\n# Build folder structure for ImageFolder-like loading\nsplit_root = f\"{work_dir}/split\"\ntrain_out = f\"{split_root}/train\"\nval_out   = f\"{split_root}/val\"\n\nfor c in [\"cat\", \"dog\"]:\n    os.makedirs(f\"{train_out}/{c}\", exist_ok=True)\n    os.makedirs(f\"{val_out}/{c}\", exist_ok=True)\n\n# Separate by label\ncats = [p for p in train_imgs if Path(p).name.startswith(\"cat.\")]\ndogs = [p for p in train_imgs if Path(p).name.startswith(\"dog.\")]\n\n# Shuffle\nrandom.shuffle(cats)\nrandom.shuffle(dogs)\n\n# 80/20 split\ndef split_list(lst, val_frac=0.2):\n    n_val = int(len(lst) * val_frac)\n    return lst[n_val:], lst[:n_val]\n\ncats_train, cats_val = split_list(cats, 0.2)\ndogs_train, dogs_val = split_list(dogs, 0.2)\n\n# Copy files\ndef copy_files(files, dest):\n    for p in files:\n        shutil.copy2(p, dest)\n\ncopy_files(cats_train, f\"{train_out}/cat\")\ncopy_files(dogs_train, f\"{train_out}/dog\")\ncopy_files(cats_val,   f\"{val_out}/cat\")\ncopy_files(dogs_val,   f\"{val_out}/dog\")\n\nprint(\"Train cats:\", len(cats_train), \"Train dogs:\", len(dogs_train))\nprint(\"Val cats:\", len(cats_val), \"Val dogs:\", len(dogs_val))\nprint(\"Split dir:\", split_root)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:29.085798Z","iopub.execute_input":"2025-12-06T20:25:29.086056Z","iopub.status.idle":"2025-12-06T20:25:42.019222Z","shell.execute_reply.started":"2025-12-06T20:25:29.086033Z","shell.execute_reply":"2025-12-06T20:25:42.018491Z"}},"outputs":[{"name":"stdout","text":"Train cats: 10000 Train dogs: 10000\nVal cats: 2500 Val dogs: 2500\nSplit dir: /kaggle/working/dogs_cats/split\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"Train cats: 10000 Train dogs: 10000\nVal cats: 2500 Val dogs: 2500\nSplit dir: /kaggle/working/dogs_cats/split","metadata":{}},{"cell_type":"code","source":"!pip -q install timm albumentations opencv-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:42.020659Z","iopub.execute_input":"2025-12-06T20:25:42.020936Z","iopub.status.idle":"2025-12-06T20:25:45.458800Z","shell.execute_reply.started":"2025-12-06T20:25:42.020917Z","shell.execute_reply":"2025-12-06T20:25:45.457907Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import os, zipfile, random, shutil, glob, math, time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport cv2\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport timm\nfrom timm.data import Mixup\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import ModelEmaV2\nfrom timm.scheduler import CosineLRScheduler\n\nprint(\"CUDA:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU:\", torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:45.459872Z","iopub.execute_input":"2025-12-06T20:25:45.460178Z","iopub.status.idle":"2025-12-06T20:25:45.467502Z","shell.execute_reply.started":"2025-12-06T20:25:45.460151Z","shell.execute_reply":"2025-12-06T20:25:45.466717Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"CUDA: True\nGPU: Tesla P100-PCIE-16GB","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 3 - Config\n# =========================\n@dataclass\nclass CFG:\n    # Kaggle input zips\n    train_zip: str = \"/kaggle/input/dogs-vs-cats/train.zip\"\n    test_zip: str  = \"/kaggle/input/dogs-vs-cats/test1.zip\"\n\n    # Working dirs\n    work_dir: str = \"/kaggle/working/dogs_cats\"\n    split_root: str = \"/kaggle/working/dogs_cats/split\"\n\n    # Data\n    img_size: int = 224\n    num_classes: int = 2\n\n    # Model: try \"convnext_tiny\", \"convnext_small\", \"convnext_base\"\n    model_name: str = \"convnext_small\"\n    drop_path_rate: float = 0.05\n\n    # Training\n    epochs: int = 100\n    batch_size: int = 64   # good старт for small; reduce if OOM\n    num_workers: int = 6\n\n    # Optim\n    lr: float = 3e-4\n    weight_decay: float = 0.05\n\n    # Scheduler\n    warmup_epochs: int = 5\n    min_lr: float = 1e-6\n\n    # Mixup / CutMix (moderate)\n    mixup_alpha: float = 0.1\n    cutmix_alpha: float = 0.2\n    mix_prob: float = 1.0\n    switch_prob: float = 0.5\n    mix_mode: str = \"batch\"\n    \n    # Delay Mixup/CutMix to improve peak accuracy\n    mix_start_epoch: int = 5\n    \n    # Label smoothing (used also by Mixup helper for targets)\n    label_smoothing: float = 0.05\n\n    # EMA\n    use_ema: bool = False\n    ema_decay: float = 0.9999\n    \n    # AMP\n    amp: bool = True\n\n    seed: int = 42\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    log_every: int = 50\n    save_path: str = \"/kaggle/working/best_ema.pt\"\n\ncfg = CFG()\ncfg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:45.468385Z","iopub.execute_input":"2025-12-06T20:25:45.468666Z","iopub.status.idle":"2025-12-06T20:25:45.485082Z","shell.execute_reply.started":"2025-12-06T20:25:45.468645Z","shell.execute_reply":"2025-12-06T20:25:45.484404Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"CFG(train_zip='/kaggle/input/dogs-vs-cats/train.zip', test_zip='/kaggle/input/dogs-vs-cats/test1.zip', work_dir='/kaggle/working/dogs_cats', split_root='/kaggle/working/dogs_cats/split', img_size=224, num_classes=2, model_name='convnext_small', drop_path_rate=0.05, epochs=100, batch_size=64, num_workers=6, lr=0.0003, weight_decay=0.05, warmup_epochs=5, min_lr=1e-06, mixup_alpha=0.1, cutmix_alpha=0.2, mix_prob=1.0, switch_prob=0.5, mix_mode='batch', mix_start_epoch=5, label_smoothing=0.05, use_ema=False, ema_decay=0.9999, amp=True, seed=42, device='cuda', log_every=50, save_path='/kaggle/working/best_ema.pt')"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"CFG(train_zip='/kaggle/input/dogs-vs-cats/train.zip', test_zip='/kaggle/input/dogs-vs-cats/test1.zip', work_dir='/kaggle/working/dogs_cats', split_root='/kaggle/working/dogs_cats/split', img_size=224, num_classes=2, model_name='convnext_small', drop_path_rate=0.05, epochs=100, batch_size=64, num_workers=6, lr=0.0003, weight_decay=0.05, warmup_epochs=5, min_lr=1e-06, mixup_alpha=0.1, cutmix_alpha=0.2, mix_prob=1.0, switch_prob=0.5, mix_mode='batch', mix_start_epoch=5, label_smoothing=0.05, use_ema=False, ema_decay=0.9999, amp=True, seed=42, device='cuda', log_every=50, save_path='/kaggle/working/best_ema.pt')","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 4 - Repro + speed\n# =========================\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(cfg.seed)\ntorch.backends.cudnn.benchmark = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:45.485959Z","iopub.execute_input":"2025-12-06T20:25:45.486293Z","iopub.status.idle":"2025-12-06T20:25:45.500229Z","shell.execute_reply.started":"2025-12-06T20:25:45.486268Z","shell.execute_reply":"2025-12-06T20:25:45.499711Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# =========================\n# Cell 5 - Prepare data from zips + make your exact split\n#         (10000 train + 2500 val per class)\n# =========================\ndef prepare_split_12500_per_class(cfg: CFG, rebuild: bool = False):\n    raw_train = f\"{cfg.work_dir}/raw_train\"\n    raw_test  = f\"{cfg.work_dir}/raw_test\"\n\n    train_out = f\"{cfg.split_root}/train\"\n    val_out   = f\"{cfg.split_root}/val\"\n\n    # If already prepared, skip unless rebuild=True\n    if (not rebuild) and os.path.isdir(cfg.split_root) and \\\n       os.path.isdir(f\"{train_out}/cat\") and os.path.isdir(f\"{val_out}/dog\"):\n        # quick count sanity\n        train_c = len(glob.glob(f\"{train_out}/cat/*\"))\n        train_d = len(glob.glob(f\"{train_out}/dog/*\"))\n        val_c   = len(glob.glob(f\"{val_out}/cat/*\"))\n        val_d   = len(glob.glob(f\"{val_out}/dog/*\"))\n        print(\"Split already exists.\")\n        print(\"Train cats:\", train_c, \"Train dogs:\", train_d)\n        print(\"Val cats:\", val_c, \"Val dogs:\", val_d)\n        return cfg.split_root\n\n    # Clean old\n    if os.path.isdir(cfg.work_dir):\n        shutil.rmtree(cfg.work_dir)\n    os.makedirs(raw_train, exist_ok=True)\n    os.makedirs(raw_test, exist_ok=True)\n\n    # Unzip train/test\n    with zipfile.ZipFile(cfg.train_zip, 'r') as z:\n        z.extractall(raw_train)\n\n    with zipfile.ZipFile(cfg.test_zip, 'r') as z:\n        z.extractall(raw_test)\n\n    # Train images path in this dataset\n    train_imgs = sorted(glob.glob(f\"{raw_train}/train/*.jpg\"))\n    if len(train_imgs) == 0:\n        raise RuntimeError(\"No train images found after unzip. Check zip structure.\")\n\n    # Create folders\n    for c in [\"cat\", \"dog\"]:\n        os.makedirs(f\"{train_out}/{c}\", exist_ok=True)\n        os.makedirs(f\"{val_out}/{c}\", exist_ok=True)\n\n    # Separate by label\n    cats = [p for p in train_imgs if Path(p).name.startswith(\"cat.\")]\n    dogs = [p for p in train_imgs if Path(p).name.startswith(\"dog.\")]\n\n    random.shuffle(cats)\n    random.shuffle(dogs)\n\n    # Ensure we have at least 12,500 per class\n    need = 12500\n    if len(cats) < need or len(dogs) < need:\n        raise RuntimeError(f\"Not enough images per class for 12,500 cap. \"\n                           f\"Cats={len(cats)}, Dogs={len(dogs)}\")\n\n    cats = cats[:need]\n    dogs = dogs[:need]\n\n    # 10,000 train + 2,500 val\n    cats_train, cats_val = cats[:10000], cats[10000:]\n    dogs_train, dogs_val = dogs[:10000], dogs[10000:]\n\n    def copy_files(files, dest):\n        for p in files:\n            shutil.copy2(p, dest)\n\n    copy_files(cats_train, f\"{train_out}/cat\")\n    copy_files(dogs_train, f\"{train_out}/dog\")\n    copy_files(cats_val,   f\"{val_out}/cat\")\n    copy_files(dogs_val,   f\"{val_out}/dog\")\n\n    print(\"Train cats:\", len(cats_train), \"Train dogs:\", len(dogs_train))\n    print(\"Val cats:\", len(cats_val), \"Val dogs:\", len(dogs_val))\n    print(\"Split dir:\", cfg.split_root)\n\n    return cfg.split_root\n\nprepare_split_12500_per_class(cfg, rebuild=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:45.501153Z","iopub.execute_input":"2025-12-06T20:25:45.501387Z","iopub.status.idle":"2025-12-06T20:25:45.571086Z","shell.execute_reply.started":"2025-12-06T20:25:45.501370Z","shell.execute_reply":"2025-12-06T20:25:45.570303Z"}},"outputs":[{"name":"stdout","text":"Split already exists.\nTrain cats: 10000 Train dogs: 10000\nVal cats: 2500 Val dogs: 2500\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/dogs_cats/split'"},"metadata":{}}],"execution_count":45},{"cell_type":"markdown","source":"Split already exists.\nTrain cats: 10000 Train dogs: 10000\nVal cats: 2500 Val dogs: 2500\n'/kaggle/working/dogs_cats/split'","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 6 - Dataset + Albumentations\n# =========================\nclass CatsDogsDataset(Dataset):\n    def __init__(self, root: str, split: str, transform=None):\n        self.transform = transform\n        split_dir = os.path.join(root, split)\n        class_names = [\"cat\", \"dog\"]\n\n        self.samples: List[Tuple[str, int]] = []\n        for idx, cname in enumerate(class_names):\n            cdir = os.path.join(split_dir, cname)\n            if not os.path.isdir(cdir):\n                continue\n            for fname in os.listdir(cdir):\n                fpath = os.path.join(cdir, fname)\n                if os.path.isfile(fpath):\n                    self.samples.append((fpath, idx))\n\n        if len(self.samples) == 0:\n            raise RuntimeError(\n                f\"No images found in {split_dir}. Expected 'cat' and 'dog' subfolders.\"\n            )\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, i):\n        path, label = self.samples[i]\n        img = cv2.imread(path)\n        if img is None:\n            img = np.zeros((cfg.img_size, cfg.img_size, 3), dtype=np.uint8)\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            img = self.transform(image=img)[\"image\"]\n\n        return img, label\n\n\ndef build_transforms(img_size: int):\n    mean = (0.485, 0.456, 0.406)\n    std  = (0.229, 0.224, 0.225)\n\n    train_tf = A.Compose([\n        A.RandomResizedCrop(\n            size=(img_size, img_size),\n            scale=(0.7, 1.0),\n            ratio=(0.75, 1.33),\n            p=1.0\n        ),\n        A.HorizontalFlip(p=0.5),\n        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n        A.RandomBrightnessContrast(p=0.3),\n        A.HueSaturationValue(p=0.3),\n        A.GaussianBlur(blur_limit=(3, 5), p=0.1),\n        A.CoarseDropout(\n            num_holes_range=(1, 8),\n            hole_height_range=(0.05, 0.12),  # fraction of image height\n            hole_width_range=(0.05, 0.12),   # fraction of image width\n            fill=0,\n            p=0.2\n        ),\n        A.Normalize(mean=mean, std=std),\n            ToTensorV2(),\n    ])\n\n    val_tf = A.Compose([\n        A.Resize(height=img_size + 32, width=img_size + 32),\n        A.CenterCrop(height=img_size, width=img_size),\n        A.Normalize(mean=mean, std=std),\n        ToTensorV2(),\n    ])\n\n    return train_tf, val_tf\n\n\n\ntrain_tf, val_tf = build_transforms(cfg.img_size)\n\ntrain_ds = CatsDogsDataset(cfg.split_root, \"train\", transform=train_tf)\nval_ds   = CatsDogsDataset(cfg.split_root, \"val\",   transform=val_tf)\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=cfg.batch_size, shuffle=True,\n    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n)\nval_loader = DataLoader(\n    val_ds, batch_size=cfg.batch_size * 2, shuffle=False,\n    num_workers=cfg.num_workers, pin_memory=True\n)\n\nprint(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))\nx, y = next(iter(train_loader))\nprint(\"Batch:\", x.shape, y[:8].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:45.573400Z","iopub.execute_input":"2025-12-06T20:25:45.573708Z","iopub.status.idle":"2025-12-06T20:25:47.073946Z","shell.execute_reply.started":"2025-12-06T20:25:45.573690Z","shell.execute_reply":"2025-12-06T20:25:47.073158Z"}},"outputs":[{"name":"stdout","text":"Train size: 20000 Val size: 5000\nBatch: torch.Size([64, 3, 224, 224]) [0, 0, 1, 0, 1, 0, 0, 1]\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"Train size: 20000 Val size: 5000\nBatch: torch.Size([64, 3, 224, 224]) [0, 0, 1, 0, 1, 0, 0, 1]","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 7 - Model, Mixup/CutMix, Loss, Optim, Scheduler, EMA\n# =========================\nmodel = timm.create_model(\n    cfg.model_name,\n    pretrained=False,\n    num_classes=cfg.num_classes,\n    drop_path_rate=cfg.drop_path_rate\n).to(cfg.device)\n\nmodel = model.to(memory_format=torch.channels_last)\n  \noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=cfg.lr,\n    weight_decay=cfg.weight_decay\n)\n\n# Mixup/CutMix helper\nmixup_fn = None\nif cfg.mixup_alpha > 0 or cfg.cutmix_alpha > 0:\n    mixup_fn = Mixup(\n        mixup_alpha=cfg.mixup_alpha,\n        cutmix_alpha=cfg.cutmix_alpha,\n        prob=cfg.mix_prob,\n        switch_prob=cfg.switch_prob,\n        mode=cfg.mix_mode,\n        label_smoothing=cfg.label_smoothing,\n        num_classes=cfg.num_classes\n    )\n\n# Training loss\ntrain_loss_fn = SoftTargetCrossEntropy() if mixup_fn is not None \\\n                else LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\n\n# Validation loss (hard labels)\nval_loss_fn = nn.CrossEntropyLoss()\n\n# EMA\nema = ModelEmaV2(model, decay=cfg.ema_decay) if cfg.use_ema else None\n\n# Scheduler: warmup + cosine over updates\nsteps_per_epoch = len(train_loader)\ntotal_updates = cfg.epochs * steps_per_epoch\nwarmup_updates = cfg.warmup_epochs * steps_per_epoch\n\nscheduler = CosineLRScheduler(\n    optimizer,\n    t_initial=total_updates,\n    lr_min=cfg.min_lr,\n    warmup_t=warmup_updates,\n    warmup_lr_init=cfg.min_lr,\n    cycle_limit=1,\n    t_in_epochs=False\n)\n\nhard_train_loss_fn = LabelSmoothingCrossEntropy(smoothing=cfg.label_smoothing)\nsoft_train_loss_fn = SoftTargetCrossEntropy()\n\nscaler = torch.amp.GradScaler(\"cuda\", enabled=(cfg.amp and cfg.device.startswith(\"cuda\")))\n\nprint(\"Model:\", cfg.model_name)\nprint(\"Params (M):\", sum(p.numel() for p in model.parameters()) / 1e6)\nprint(\"Mixup/CutMix:\", \"ON\" if mixup_fn is not None else \"OFF\")\nprint(\"Total updates:\", total_updates, \"Warmup updates:\", warmup_updates)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:47.075090Z","iopub.execute_input":"2025-12-06T20:25:47.075312Z","iopub.status.idle":"2025-12-06T20:25:47.949951Z","shell.execute_reply.started":"2025-12-06T20:25:47.075290Z","shell.execute_reply":"2025-12-06T20:25:47.949075Z"}},"outputs":[{"name":"stdout","text":"Model: convnext_small\nParams (M): 49.456226\nMixup/CutMix: ON\nTotal updates: 31200 Warmup updates: 1560\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"Model: convnext_small\nParams (M): 49.456226\nMixup/CutMix: ON\nTotal updates: 31200 Warmup updates: 1560","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 8 - Train / Eval loops\n# =========================\ndef accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    preds = torch.argmax(logits, dim=1)\n    return (preds == targets).float().mean().item()\n\n\ndef train_one_epoch(epoch: int):\n    model.train()\n    running_loss, running_acc, n = 0.0, 0.0, 0\n\n    active_mixup = mixup_fn if (mixup_fn is not None and epoch >= cfg.mix_start_epoch) else None\n\n    for step, (images, targets) in enumerate(train_loader):\n        images = images.to(cfg.device, non_blocking=True)\n        targets = targets.to(cfg.device, non_blocking=True)\n\n        if active_mixup is not None:\n            images, targets = active_mixup(images, targets)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast(\"cuda\", enabled=scaler.is_enabled()):\n            logits = model(images)\n            loss = soft_train_loss_fn(logits, targets) if active_mixup is not None \\\n                   else hard_train_loss_fn(logits, targets)\n\n        if scaler.is_enabled():\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            optimizer.step()\n\n        if ema is not None:\n            ema.update(model)\n\n        global_update = epoch * steps_per_epoch + step + 1\n        scheduler.step_update(global_update)\n\n        bs = images.size(0)\n        running_loss += loss.item() * bs\n        n += bs\n\n        # only meaningful without mixup\n        if active_mixup is None:\n            running_acc += (logits.argmax(1) == targets).float().sum().item()\n\n        if (step + 1) % cfg.log_every == 0:\n            avg_loss = running_loss / n\n            if active_mixup is None:\n                avg_acc = running_acc / n\n                print(f\"  step {step+1}/{steps_per_epoch} - loss {avg_loss:.4f} - acc {avg_acc:.4f}\")\n            else:\n                print(f\"  step {step+1}/{steps_per_epoch} - loss {avg_loss:.4f}\")\n\n    epoch_loss = running_loss / n\n    epoch_acc = (running_acc / n) if active_mixup is None else float(\"nan\")\n    return epoch_loss, epoch_acc\n\n@torch.no_grad()\ndef evaluate(eval_model):\n    eval_model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    n = 0\n\n    for images, targets in val_loader:\n        images = images.to(cfg.device, non_blocking=True).to(memory_format=torch.channels_last)\n\n        targets = targets.to(cfg.device, non_blocking=True)\n\n        logits = eval_model(images)\n        loss = val_loss_fn(logits, targets)\n\n        bs = images.size(0)\n        running_loss += loss.item() * bs\n        running_acc += accuracy(logits, targets) * bs\n        n += bs\n\n    return running_loss / n, running_acc / n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:47.950840Z","iopub.execute_input":"2025-12-06T20:25:47.951058Z","iopub.status.idle":"2025-12-06T20:25:47.962994Z","shell.execute_reply.started":"2025-12-06T20:25:47.951034Z","shell.execute_reply":"2025-12-06T20:25:47.962109Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# =========================\n# Cell 9 - Training run\n# =========================\nbest_acc = 0.0\n\nprint(\"Device:\", cfg.device)\nprint(f\"Train images: {len(train_ds)} | Val images: {len(val_ds)}\")\nprint(\"\")\n\nfor epoch in range(cfg.epochs):\n    print({\n        \"model\": cfg.model_name,\n        \"lr\": cfg.lr,\n        \"batch\": cfg.batch_size,\n        \"drop_path\": cfg.drop_path_rate,\n        \"label_smoothing\": cfg.label_smoothing,\n        \"mixup_alpha\": cfg.mixup_alpha,\n        \"cutmix_alpha\": cfg.cutmix_alpha,\n    })\n    print(f\"Epoch {epoch+1}/{cfg.epochs}\")\n\n    t0 = time.time()\n    train_loss, train_acc = train_one_epoch(epoch)\n\n    eval_model = ema.module if ema is not None else model\n    val_loss, val_acc = evaluate(eval_model)\n\n    dt = time.time() - t0\n\n    if math.isnan(train_acc):\n        print(f\"  train loss {train_loss:.4f} | val loss {val_loss:.4f} | val acc {val_acc:.4f} | {dt:.1f}s\")\n    else:\n        print(f\"  train loss {train_loss:.4f} | train acc {train_acc:.4f} | val loss {val_loss:.4f} | val acc {val_acc:.4f} | {dt:.1f}s\")\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save({\n            \"model_name\": cfg.model_name,\n            \"model_state\": eval_model.state_dict(),\n            \"best_acc\": best_acc,\n            \"cfg\": cfg.__dict__\n        }, cfg.save_path)\n        print(f\"  Saved best EMA checkpoint to {cfg.save_path} (acc={best_acc:.4f})\")\n    print(\"LR end of epoch:\", optimizer.param_groups[0][\"lr\"])\n    print(\"\")\n\nprint(\"Best val acc:\", best_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T20:25:47.963752Z","iopub.execute_input":"2025-12-06T20:25:47.963958Z","iopub.status.idle":"2025-12-07T06:02:25.677257Z","shell.execute_reply.started":"2025-12-06T20:25:47.963943Z","shell.execute_reply":"2025-12-07T06:02:25.676387Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Device: cuda\nTrain images: 20000 | Val images: 5000\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 1/100\n  step 50/312 - loss 0.7011 - acc 0.5041\n  step 100/312 - loss 0.7029 - acc 0.4983\n  step 150/312 - loss 0.7053 - acc 0.5018\n  step 200/312 - loss 0.7042 - acc 0.5081\n  step 250/312 - loss 0.7020 - acc 0.5134\n  step 300/312 - loss 0.7004 - acc 0.5193\n  train loss 0.6998 | train acc 0.5208 | val loss 0.6710 | val acc 0.5774 | 345.8s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.5774)\nLR end of epoch: 6.079999999999998e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 2/100\n  step 50/312 - loss 0.6814 - acc 0.5816\n  step 100/312 - loss 0.6852 - acc 0.5708\n  step 150/312 - loss 0.6852 - acc 0.5674\n  step 200/312 - loss 0.6821 - acc 0.5730\n  step 250/312 - loss 0.6798 - acc 0.5760\n  step 300/312 - loss 0.6773 - acc 0.5790\n  train loss 0.6768 | train acc 0.5793 | val loss 0.6516 | val acc 0.6018 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6018)\nLR end of epoch: 0.00012059999999999996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 3/100\n  step 50/312 - loss 0.6725 - acc 0.5928\n  step 100/312 - loss 0.6680 - acc 0.5945\n  step 150/312 - loss 0.6667 - acc 0.5986\n  step 200/312 - loss 0.6677 - acc 0.5956\n  step 250/312 - loss 0.6643 - acc 0.6018\n  step 300/312 - loss 0.6607 - acc 0.6066\n  train loss 0.6596 | train acc 0.6080 | val loss 0.6312 | val acc 0.6298 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6298)\nLR end of epoch: 0.00018039999999999997\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 4/100\n  step 50/312 - loss 0.6389 - acc 0.6409\n  step 100/312 - loss 0.6392 - acc 0.6427\n  step 150/312 - loss 0.6386 - acc 0.6405\n  step 200/312 - loss 0.6356 - acc 0.6449\n  step 250/312 - loss 0.6303 - acc 0.6519\n  step 300/312 - loss 0.6288 - acc 0.6550\n  train loss 0.6292 | train acc 0.6543 | val loss 0.6126 | val acc 0.6548 | 345.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6548)\nLR end of epoch: 0.00024019999999999993\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 5/100\n  step 50/312 - loss 0.6087 - acc 0.6778\n  step 100/312 - loss 0.6098 - acc 0.6763\n  step 150/312 - loss 0.6079 - acc 0.6773\n  step 200/312 - loss 0.6060 - acc 0.6791\n  step 250/312 - loss 0.6061 - acc 0.6789\n  step 300/312 - loss 0.6038 - acc 0.6821\n  train loss 0.6034 | train acc 0.6830 | val loss 0.5581 | val acc 0.7044 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7044)\nLR end of epoch: 0.00029815940691897306\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 6/100\n  step 50/312 - loss 0.6294\n  step 100/312 - loss 0.6345\n  step 150/312 - loss 0.6253\n  step 200/312 - loss 0.6236\n  step 250/312 - loss 0.6223\n  step 300/312 - loss 0.6209\n  train loss 0.6198 | val loss 0.5408 | val acc 0.7200 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7200)\nLR end of epoch: 0.00029735194398393893\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 7/100\n  step 50/312 - loss 0.6175\n  step 100/312 - loss 0.6143\n  step 150/312 - loss 0.6106\n  step 200/312 - loss 0.6100\n  step 250/312 - loss 0.6095\n  step 300/312 - loss 0.6091\n  train loss 0.6091 | val loss 0.5486 | val acc 0.7290 | 345.4s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7290)\nLR end of epoch: 0.0002963995559098427\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 8/100\n  step 50/312 - loss 0.6070\n  step 100/312 - loss 0.6024\n  step 150/312 - loss 0.6050\n  step 200/312 - loss 0.6006\n  step 250/312 - loss 0.5999\n  step 300/312 - loss 0.5997\n  train loss 0.5989 | val loss 0.5187 | val acc 0.7398 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7398)\nLR end of epoch: 0.00029530318258873034\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 9/100\n  step 50/312 - loss 0.5810\n  step 100/312 - loss 0.5839\n  step 150/312 - loss 0.5810\n  step 200/312 - loss 0.5853\n  step 250/312 - loss 0.5879\n  step 300/312 - loss 0.5872\n  train loss 0.5876 | val loss 0.4951 | val acc 0.7626 | 345.4s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7626)\nLR end of epoch: 0.00029406390600870296\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 10/100\n  step 50/312 - loss 0.5791\n  step 100/312 - loss 0.5770\n  step 150/312 - loss 0.5786\n  step 200/312 - loss 0.5746\n  step 250/312 - loss 0.5741\n  step 300/312 - loss 0.5751\n  train loss 0.5750 | val loss 0.4838 | val acc 0.7764 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7764)\nLR end of epoch: 0.0002926829491861254\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 11/100\n  step 50/312 - loss 0.5487\n  step 100/312 - loss 0.5603\n  step 150/312 - loss 0.5678\n  step 200/312 - loss 0.5675\n  step 250/312 - loss 0.5635\n  step 300/312 - loss 0.5604\n  train loss 0.5610 | val loss 0.4754 | val acc 0.7814 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7814)\nLR end of epoch: 0.0002911616749586567\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 12/100\n  step 50/312 - loss 0.5760\n  step 100/312 - loss 0.5639\n  step 150/312 - loss 0.5616\n  step 200/312 - loss 0.5612\n  step 250/312 - loss 0.5612\n  step 300/312 - loss 0.5618\n  train loss 0.5622 | val loss 0.4590 | val acc 0.7882 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7882)\nLR end of epoch: 0.00028950158464029354\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 13/100\n  step 50/312 - loss 0.5363\n  step 100/312 - loss 0.5427\n  step 150/312 - loss 0.5505\n  step 200/312 - loss 0.5527\n  step 250/312 - loss 0.5508\n  step 300/312 - loss 0.5481\n  train loss 0.5491 | val loss 0.4381 | val acc 0.8144 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8144)\nLR end of epoch: 0.00028770431653975514\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 14/100\n  step 50/312 - loss 0.5292\n  step 100/312 - loss 0.5346\n  step 150/312 - loss 0.5318\n  step 200/312 - loss 0.5318\n  step 250/312 - loss 0.5329\n  step 300/312 - loss 0.5328\n  train loss 0.5307 | val loss 0.4054 | val acc 0.8142 | 345.9s\nLR end of epoch: 0.0002857716443436699\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 15/100\n  step 50/312 - loss 0.5251\n  step 100/312 - loss 0.5260\n  step 150/312 - loss 0.5283\n  step 200/312 - loss 0.5286\n  step 250/312 - loss 0.5295\n  step 300/312 - loss 0.5320\n  train loss 0.5318 | val loss 0.3795 | val acc 0.8394 | 345.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8394)\nLR end of epoch: 0.00028370547536616097\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 16/100\n  step 50/312 - loss 0.5167\n  step 100/312 - loss 0.5213\n  step 150/312 - loss 0.5203\n  step 200/312 - loss 0.5147\n  step 250/312 - loss 0.5155\n  step 300/312 - loss 0.5167\n  train loss 0.5166 | val loss 0.3836 | val acc 0.8386 | 346.0s\nLR end of epoch: 0.00028150784866655756\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 17/100\n  step 50/312 - loss 0.5131\n  step 100/312 - loss 0.5249\n  step 150/312 - loss 0.5148\n  step 200/312 - loss 0.5096\n  step 250/312 - loss 0.5063\n  step 300/312 - loss 0.5065\n  train loss 0.5066 | val loss 0.3782 | val acc 0.8368 | 345.5s\nLR end of epoch: 0.00027918093303708956\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 18/100\n  step 50/312 - loss 0.4860\n  step 100/312 - loss 0.4884\n  step 150/312 - loss 0.4869\n  step 200/312 - loss 0.4857\n  step 250/312 - loss 0.4880\n  step 300/312 - loss 0.4903\n  train loss 0.4905 | val loss 0.3670 | val acc 0.8464 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8464)\nLR end of epoch: 0.00027672702486255125\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 19/100\n  step 50/312 - loss 0.5028\n  step 100/312 - loss 0.5082\n  step 150/312 - loss 0.5009\n  step 200/312 - loss 0.4999\n  step 250/312 - loss 0.4960\n  step 300/312 - loss 0.4937\n  train loss 0.4935 | val loss 0.3492 | val acc 0.8524 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8524)\nLR end of epoch: 0.00027414854585404696\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 20/100\n  step 50/312 - loss 0.5009\n  step 100/312 - loss 0.5007\n  step 150/312 - loss 0.4956\n  step 200/312 - loss 0.4918\n  step 250/312 - loss 0.4890\n  step 300/312 - loss 0.4819\n  train loss 0.4807 | val loss 0.3250 | val acc 0.8588 | 346.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8588)\nLR end of epoch: 0.0002714480406590546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 21/100\n  step 50/312 - loss 0.4629\n  step 100/312 - loss 0.4682\n  step 150/312 - loss 0.4720\n  step 200/312 - loss 0.4771\n  step 250/312 - loss 0.4771\n  step 300/312 - loss 0.4728\n  train loss 0.4729 | val loss 0.3101 | val acc 0.8762 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8762)\nLR end of epoch: 0.0002686281743501657\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 22/100\n  step 50/312 - loss 0.4863\n  step 100/312 - loss 0.4869\n  step 150/312 - loss 0.4784\n  step 200/312 - loss 0.4734\n  step 250/312 - loss 0.4692\n  step 300/312 - loss 0.4669\n  train loss 0.4659 | val loss 0.2996 | val acc 0.8782 | 346.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8782)\nLR end of epoch: 0.00026569172979498044\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 23/100\n  step 50/312 - loss 0.4712\n  step 100/312 - loss 0.4584\n  step 150/312 - loss 0.4620\n  step 200/312 - loss 0.4650\n  step 250/312 - loss 0.4721\n  step 300/312 - loss 0.4731\n  train loss 0.4724 | val loss 0.3156 | val acc 0.8782 | 346.0s\nLR end of epoch: 0.00026264160490975367\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 24/100\n  step 50/312 - loss 0.4277\n  step 100/312 - loss 0.4312\n  step 150/312 - loss 0.4425\n  step 200/312 - loss 0.4329\n  step 250/312 - loss 0.4338\n  step 300/312 - loss 0.4346\n  train loss 0.4310 | val loss 0.2743 | val acc 0.8844 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8844)\nLR end of epoch: 0.00025948080979950103\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 25/100\n  step 50/312 - loss 0.4628\n  step 100/312 - loss 0.4471\n  step 150/312 - loss 0.4488\n  step 200/312 - loss 0.4456\n  step 250/312 - loss 0.4498\n  step 300/312 - loss 0.4517\n  train loss 0.4529 | val loss 0.2885 | val acc 0.8874 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8874)\nLR end of epoch: 0.00025621246378738883\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 26/100\n  step 50/312 - loss 0.4264\n  step 100/312 - loss 0.4309\n  step 150/312 - loss 0.4430\n  step 200/312 - loss 0.4359\n  step 250/312 - loss 0.4368\n  step 300/312 - loss 0.4327\n  train loss 0.4320 | val loss 0.2716 | val acc 0.8948 | 346.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8948)\nLR end of epoch: 0.00025283979233633894\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 27/100\n  step 50/312 - loss 0.4492\n  step 100/312 - loss 0.4252\n  step 150/312 - loss 0.4238\n  step 200/312 - loss 0.4232\n  step 250/312 - loss 0.4268\n  step 300/312 - loss 0.4282\n  train loss 0.4286 | val loss 0.2910 | val acc 0.8888 | 345.3s\nLR end of epoch: 0.0002493661238658859\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 28/100\n  step 50/312 - loss 0.4045\n  step 100/312 - loss 0.4247\n  step 150/312 - loss 0.4139\n  step 200/312 - loss 0.4102\n  step 250/312 - loss 0.4087\n  step 300/312 - loss 0.4121\n  train loss 0.4099 | val loss 0.2486 | val acc 0.9096 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9096)\nLR end of epoch: 0.0002457948864674291\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 29/100\n  step 50/312 - loss 0.4183\n  step 100/312 - loss 0.4148\n  step 150/312 - loss 0.4164\n  step 200/312 - loss 0.4174\n  step 250/312 - loss 0.4206\n  step 300/312 - loss 0.4255\n  train loss 0.4241 | val loss 0.2440 | val acc 0.9076 | 346.2s\nLR end of epoch: 0.00024212960452111996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 30/100\n  step 50/312 - loss 0.3939\n  step 100/312 - loss 0.4042\n  step 150/312 - loss 0.4014\n  step 200/312 - loss 0.4067\n  step 250/312 - loss 0.4096\n  step 300/312 - loss 0.4103\n  train loss 0.4106 | val loss 0.2395 | val acc 0.9158 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9158)\nLR end of epoch: 0.0002383738952177247\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 31/100\n  step 50/312 - loss 0.4266\n  step 100/312 - loss 0.4132\n  step 150/312 - loss 0.4040\n  step 200/312 - loss 0.4067\n  step 250/312 - loss 0.4080\n  step 300/312 - loss 0.4072\n  train loss 0.4081 | val loss 0.2431 | val acc 0.9118 | 345.9s\nLR end of epoch: 0.00023453146498889348\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 32/100\n  step 50/312 - loss 0.3986\n  step 100/312 - loss 0.4087\n  step 150/312 - loss 0.3906\n  step 200/312 - loss 0.3893\n  step 250/312 - loss 0.3909\n  step 300/312 - loss 0.3918\n  train loss 0.3915 | val loss 0.2279 | val acc 0.9198 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9198)\nLR end of epoch: 0.00023060610584935996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 33/100\n  step 50/312 - loss 0.3726\n  step 100/312 - loss 0.4144\n  step 150/312 - loss 0.3979\n  step 200/312 - loss 0.3877\n  step 250/312 - loss 0.3907\n  step 300/312 - loss 0.3938\n  train loss 0.3924 | val loss 0.2069 | val acc 0.9214 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9214)\nLR end of epoch: 0.00022660169165468046\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 34/100\n  step 50/312 - loss 0.3742\n  step 100/312 - loss 0.3914\n  step 150/312 - loss 0.3948\n  step 200/312 - loss 0.3935\n  step 250/312 - loss 0.3934\n  step 300/312 - loss 0.3953\n  train loss 0.3939 | val loss 0.2108 | val acc 0.9166 | 345.4s\nLR end of epoch: 0.00022252217427820638\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 35/100\n  step 50/312 - loss 0.3772\n  step 100/312 - loss 0.3718\n  step 150/312 - loss 0.3777\n  step 200/312 - loss 0.3774\n  step 250/312 - loss 0.3722\n  step 300/312 - loss 0.3748\n  train loss 0.3734 | val loss 0.2064 | val acc 0.9266 | 345.9s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9266)\nLR end of epoch: 0.0002183715797110622\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 36/100\n  step 50/312 - loss 0.3523\n  step 100/312 - loss 0.3615\n  step 150/312 - loss 0.3721\n  step 200/312 - loss 0.3758\n  step 250/312 - loss 0.3755\n  step 300/312 - loss 0.3718\n  train loss 0.3710 | val loss 0.2158 | val acc 0.9204 | 345.7s\nLR end of epoch: 0.00021415400408897832\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 37/100\n  step 50/312 - loss 0.3398\n  step 100/312 - loss 0.3676\n  step 150/312 - loss 0.3705\n  step 200/312 - loss 0.3724\n  step 250/312 - loss 0.3772\n  step 300/312 - loss 0.3776\n  train loss 0.3783 | val loss 0.2242 | val acc 0.9258 | 345.8s\nLR end of epoch: 0.00020987360964989964\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 38/100\n  step 50/312 - loss 0.3561\n  step 100/312 - loss 0.3710\n  step 150/312 - loss 0.3736\n  step 200/312 - loss 0.3844\n  step 250/312 - loss 0.3754\n  step 300/312 - loss 0.3720\n  train loss 0.3719 | val loss 0.2174 | val acc 0.9258 | 346.2s\nLR end of epoch: 0.0002055346206263593\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 39/100\n  step 50/312 - loss 0.3524\n  step 100/312 - loss 0.3571\n  step 150/312 - loss 0.3668\n  step 200/312 - loss 0.3682\n  step 250/312 - loss 0.3652\n  step 300/312 - loss 0.3570\n  train loss 0.3586 | val loss 0.1923 | val acc 0.9322 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9322)\nLR end of epoch: 0.00020114131907667107\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 40/100\n  step 50/312 - loss 0.3368\n  step 100/312 - loss 0.3618\n  step 150/312 - loss 0.3554\n  step 200/312 - loss 0.3606\n  step 250/312 - loss 0.3628\n  step 300/312 - loss 0.3587\n  train loss 0.3608 | val loss 0.2117 | val acc 0.9296 | 345.8s\nLR end of epoch: 0.0001966980406590546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 41/100\n  step 50/312 - loss 0.3372\n  step 100/312 - loss 0.3483\n  step 150/312 - loss 0.3538\n  step 200/312 - loss 0.3591\n  step 250/312 - loss 0.3559\n  step 300/312 - loss 0.3551\n  train loss 0.3557 | val loss 0.2047 | val acc 0.9304 | 344.8s\nLR end of epoch: 0.00019220917035286475\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 42/100\n  step 50/312 - loss 0.3521\n  step 100/312 - loss 0.3497\n  step 150/312 - loss 0.3422\n  step 200/312 - loss 0.3529\n  step 250/312 - loss 0.3566\n  step 300/312 - loss 0.3580\n  train loss 0.3573 | val loss 0.1884 | val acc 0.9376 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9376)\nLR end of epoch: 0.00018767913813114574\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 43/100\n  step 50/312 - loss 0.3162\n  step 100/312 - loss 0.3412\n  step 150/312 - loss 0.3425\n  step 200/312 - loss 0.3473\n  step 250/312 - loss 0.3447\n  step 300/312 - loss 0.3478\n  train loss 0.3466 | val loss 0.2066 | val acc 0.9280 | 345.5s\nLR end of epoch: 0.00018311241458878307\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 44/100\n  step 50/312 - loss 0.3406\n  step 100/312 - loss 0.3546\n  step 150/312 - loss 0.3547\n  step 200/312 - loss 0.3566\n  step 250/312 - loss 0.3554\n  step 300/312 - loss 0.3509\n  train loss 0.3490 | val loss 0.1781 | val acc 0.9342 | 345.9s\nLR end of epoch: 0.0001785135065305658\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 45/100\n  step 50/312 - loss 0.3345\n  step 100/312 - loss 0.3523\n  step 150/312 - loss 0.3562\n  step 200/312 - loss 0.3607\n  step 250/312 - loss 0.3660\n  step 300/312 - loss 0.3595\n  train loss 0.3605 | val loss 0.2097 | val acc 0.9288 | 345.1s\nLR end of epoch: 0.00017388695252351449\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 46/100\n  step 50/312 - loss 0.3409\n  step 100/312 - loss 0.3257\n  step 150/312 - loss 0.3325\n  step 200/312 - loss 0.3344\n  step 250/312 - loss 0.3467\n  step 300/312 - loss 0.3489\n  train loss 0.3499 | val loss 0.1986 | val acc 0.9376 | 346.2s\nLR end of epoch: 0.00016923731841786345\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 47/100\n  step 50/312 - loss 0.3298\n  step 100/312 - loss 0.3383\n  step 150/312 - loss 0.3474\n  step 200/312 - loss 0.3440\n  step 250/312 - loss 0.3450\n  step 300/312 - loss 0.3499\n  train loss 0.3492 | val loss 0.1799 | val acc 0.9342 | 345.9s\nLR end of epoch: 0.0001645691928411179\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 48/100\n  step 50/312 - loss 0.3051\n  step 100/312 - loss 0.3186\n  step 150/312 - loss 0.3258\n  step 200/312 - loss 0.3345\n  step 250/312 - loss 0.3391\n  step 300/312 - loss 0.3346\n  train loss 0.3328 | val loss 0.1851 | val acc 0.9354 | 346.1s\nLR end of epoch: 0.00015988718266963235\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 49/100\n  step 50/312 - loss 0.2935\n  step 100/312 - loss 0.3184\n  step 150/312 - loss 0.3241\n  step 200/312 - loss 0.3320\n  step 250/312 - loss 0.3374\n  step 300/312 - loss 0.3323\n  train loss 0.3341 | val loss 0.1861 | val acc 0.9394 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9394)\nLR end of epoch: 0.00015519590848218014\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 50/100\n  step 50/312 - loss 0.3216\n  step 100/312 - loss 0.3340\n  step 150/312 - loss 0.3340\n  step 200/312 - loss 0.3396\n  step 250/312 - loss 0.3392\n  step 300/312 - loss 0.3351\n  train loss 0.3368 | val loss 0.2013 | val acc 0.9342 | 345.7s\nLR end of epoch: 0.00015049999999999997\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 51/100\n  step 50/312 - loss 0.3464\n  step 100/312 - loss 0.3393\n  step 150/312 - loss 0.3353\n  step 200/312 - loss 0.3369\n  step 250/312 - loss 0.3355\n  step 300/312 - loss 0.3487\n  train loss 0.3486 | val loss 0.1921 | val acc 0.9368 | 345.7s\nLR end of epoch: 0.00014580409151781983\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 52/100\n  step 50/312 - loss 0.3946\n  step 100/312 - loss 0.3767\n  step 150/312 - loss 0.3791\n  step 200/312 - loss 0.3730\n  step 250/312 - loss 0.3691\n  step 300/312 - loss 0.3585\n  train loss 0.3580 | val loss 0.1995 | val acc 0.9344 | 346.0s\nLR end of epoch: 0.00014111281733036765\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 53/100\n  step 50/312 - loss 0.3376\n  step 100/312 - loss 0.3472\n  step 150/312 - loss 0.3343\n  step 200/312 - loss 0.3286\n  step 250/312 - loss 0.3286\n  step 300/312 - loss 0.3232\n  train loss 0.3238 | val loss 0.1814 | val acc 0.9366 | 345.4s\nLR end of epoch: 0.00013643080715888212\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 54/100\n  step 50/312 - loss 0.3423\n  step 100/312 - loss 0.3465\n  step 150/312 - loss 0.3207\n  step 200/312 - loss 0.3298\n  step 250/312 - loss 0.3256\n  step 300/312 - loss 0.3259\n  train loss 0.3250 | val loss 0.1777 | val acc 0.9430 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9430)\nLR end of epoch: 0.00013176268158213652\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 55/100\n  step 50/312 - loss 0.3803\n  step 100/312 - loss 0.3491\n  step 150/312 - loss 0.3442\n  step 200/312 - loss 0.3380\n  step 250/312 - loss 0.3354\n  step 300/312 - loss 0.3354\n  train loss 0.3362 | val loss 0.1857 | val acc 0.9420 | 345.5s\nLR end of epoch: 0.00012711304747648546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 56/100\n  step 50/312 - loss 0.3689\n  step 100/312 - loss 0.3595\n  step 150/312 - loss 0.3529\n  step 200/312 - loss 0.3360\n  step 250/312 - loss 0.3327\n  step 300/312 - loss 0.3267\n  train loss 0.3265 | val loss 0.1713 | val acc 0.9408 | 345.6s\nLR end of epoch: 0.00012248649346943414\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 57/100\n  step 50/312 - loss 0.3069\n  step 100/312 - loss 0.3204\n  step 150/312 - loss 0.3339\n  step 200/312 - loss 0.3427\n  step 250/312 - loss 0.3479\n  step 300/312 - loss 0.3500\n  train loss 0.3473 | val loss 0.1741 | val acc 0.9430 | 345.5s\nLR end of epoch: 0.0001178875854112169\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 58/100\n  step 50/312 - loss 0.3442\n  step 100/312 - loss 0.3329\n  step 150/312 - loss 0.3291\n  step 200/312 - loss 0.3267\n  step 250/312 - loss 0.3251\n  step 300/312 - loss 0.3192\n  train loss 0.3178 | val loss 0.1716 | val acc 0.9424 | 345.6s\nLR end of epoch: 0.00011332086186885422\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 59/100\n  step 50/312 - loss 0.3495\n  step 100/312 - loss 0.3154\n  step 150/312 - loss 0.3198\n  step 200/312 - loss 0.3254\n  step 250/312 - loss 0.3253\n  step 300/312 - loss 0.3232\n  train loss 0.3216 | val loss 0.1793 | val acc 0.9382 | 345.8s\nLR end of epoch: 0.00010879082964713522\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 60/100\n  step 50/312 - loss 0.2933\n  step 100/312 - loss 0.3033\n  step 150/312 - loss 0.3192\n  step 200/312 - loss 0.3285\n  step 250/312 - loss 0.3306\n  step 300/312 - loss 0.3259\n  train loss 0.3279 | val loss 0.1720 | val acc 0.9418 | 346.2s\nLR end of epoch: 0.00010430195934094535\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 61/100\n  step 50/312 - loss 0.2791\n  step 100/312 - loss 0.3080\n  step 150/312 - loss 0.3051\n  step 200/312 - loss 0.3119\n  step 250/312 - loss 0.3117\n  step 300/312 - loss 0.3140\n  train loss 0.3116 | val loss 0.1675 | val acc 0.9454 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9454)\nLR end of epoch: 9.985868092332892e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 62/100\n  step 50/312 - loss 0.2584\n  step 100/312 - loss 0.2874\n  step 150/312 - loss 0.2955\n  step 200/312 - loss 0.3020\n  step 250/312 - loss 0.3090\n  step 300/312 - loss 0.3073\n  train loss 0.3091 | val loss 0.1651 | val acc 0.9454 | 346.1s\nLR end of epoch: 9.546537937364064e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 63/100\n  step 50/312 - loss 0.3186\n  step 100/312 - loss 0.3227\n  step 150/312 - loss 0.3126\n  step 200/312 - loss 0.3338\n  step 250/312 - loss 0.3309\n  step 300/312 - loss 0.3247\n  train loss 0.3293 | val loss 0.1779 | val acc 0.9452 | 345.6s\nLR end of epoch: 9.11263903501003e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 64/100\n  step 50/312 - loss 0.3432\n  step 100/312 - loss 0.3264\n  step 150/312 - loss 0.3136\n  step 200/312 - loss 0.3160\n  step 250/312 - loss 0.3156\n  step 300/312 - loss 0.3141\n  train loss 0.3143 | val loss 0.1579 | val acc 0.9470 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9470)\nLR end of epoch: 8.684599591102167e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 65/100\n  step 50/312 - loss 0.2691\n  step 100/312 - loss 0.3118\n  step 150/312 - loss 0.3015\n  step 200/312 - loss 0.3007\n  step 250/312 - loss 0.2975\n  step 300/312 - loss 0.2968\n  train loss 0.2989 | val loss 0.1727 | val acc 0.9446 | 345.6s\nLR end of epoch: 8.262842028893775e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 66/100\n  step 50/312 - loss 0.3495\n  step 100/312 - loss 0.3441\n  step 150/312 - loss 0.3330\n  step 200/312 - loss 0.3178\n  step 250/312 - loss 0.3184\n  step 300/312 - loss 0.3124\n  train loss 0.3095 | val loss 0.1570 | val acc 0.9442 | 345.5s\nLR end of epoch: 7.84778257217936e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 67/100\n  step 50/312 - loss 0.2986\n  step 100/312 - loss 0.3086\n  step 150/312 - loss 0.3109\n  step 200/312 - loss 0.3121\n  step 250/312 - loss 0.3076\n  step 300/312 - loss 0.3080\n  train loss 0.3119 | val loss 0.1665 | val acc 0.9484 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9484)\nLR end of epoch: 7.439830834531953e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 68/100\n  step 50/312 - loss 0.3663\n  step 100/312 - loss 0.3257\n  step 150/312 - loss 0.3194\n  step 200/312 - loss 0.3085\n  step 250/312 - loss 0.3120\n  step 300/312 - loss 0.3153\n  train loss 0.3144 | val loss 0.1637 | val acc 0.9500 | 346.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9500)\nLR end of epoch: 7.039389415064002e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 69/100\n  step 50/312 - loss 0.3380\n  step 100/312 - loss 0.3121\n  step 150/312 - loss 0.3195\n  step 200/312 - loss 0.3182\n  step 250/312 - loss 0.3133\n  step 300/312 - loss 0.3139\n  train loss 0.3149 | val loss 0.1600 | val acc 0.9472 | 346.0s\nLR end of epoch: 6.646853501110649e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 70/100\n  step 50/312 - loss 0.3114\n  step 100/312 - loss 0.3189\n  step 150/312 - loss 0.3213\n  step 200/312 - loss 0.3238\n  step 250/312 - loss 0.3277\n  step 300/312 - loss 0.3279\n  train loss 0.3224 | val loss 0.1489 | val acc 0.9496 | 345.9s\nLR end of epoch: 6.262610478227527e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 71/100\n  step 50/312 - loss 0.3095\n  step 100/312 - loss 0.2875\n  step 150/312 - loss 0.3044\n  step 200/312 - loss 0.2983\n  step 250/312 - loss 0.3025\n  step 300/312 - loss 0.3010\n  train loss 0.2992 | val loss 0.1550 | val acc 0.9502 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9502)\nLR end of epoch: 5.8870395478880035e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 72/100\n  step 50/312 - loss 0.2792\n  step 100/312 - loss 0.3184\n  step 150/312 - loss 0.3278\n  step 200/312 - loss 0.3318\n  step 250/312 - loss 0.3284\n  step 300/312 - loss 0.3268\n  train loss 0.3297 | val loss 0.1733 | val acc 0.9484 | 345.6s\nLR end of epoch: 5.520511353257087e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 73/100\n  step 50/312 - loss 0.3186\n  step 100/312 - loss 0.3035\n  step 150/312 - loss 0.3140\n  step 200/312 - loss 0.3191\n  step 250/312 - loss 0.3216\n  step 300/312 - loss 0.3190\n  train loss 0.3188 | val loss 0.1660 | val acc 0.9510 | 345.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9510)\nLR end of epoch: 5.1633876134114055e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 74/100\n  step 50/312 - loss 0.2983\n  step 100/312 - loss 0.3163\n  step 150/312 - loss 0.3108\n  step 200/312 - loss 0.3166\n  step 250/312 - loss 0.3186\n  step 300/312 - loss 0.3176\n  train loss 0.3184 | val loss 0.1590 | val acc 0.9484 | 346.5s\nLR end of epoch: 4.816020766366103e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 75/100\n  step 50/312 - loss 0.3122\n  step 100/312 - loss 0.2904\n  step 150/312 - loss 0.2958\n  step 200/312 - loss 0.2950\n  step 250/312 - loss 0.3025\n  step 300/312 - loss 0.2970\n  train loss 0.2955 | val loss 0.1466 | val acc 0.9510 | 345.7s\nLR end of epoch: 4.478753621261114e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 76/100\n  step 50/312 - loss 0.3129\n  step 100/312 - loss 0.3099\n  step 150/312 - loss 0.3122\n  step 200/312 - loss 0.3117\n  step 250/312 - loss 0.3001\n  step 300/312 - loss 0.3078\n  train loss 0.3066 | val loss 0.1529 | val acc 0.9512 | 345.9s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9512)\nLR end of epoch: 4.1519190200498946e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 77/100\n  step 50/312 - loss 0.3234\n  step 100/312 - loss 0.2983\n  step 150/312 - loss 0.2982\n  step 200/312 - loss 0.3094\n  step 250/312 - loss 0.3062\n  step 300/312 - loss 0.3017\n  train loss 0.3016 | val loss 0.1512 | val acc 0.9526 | 345.8s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9526)\nLR end of epoch: 3.835839509024628e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 78/100\n  step 50/312 - loss 0.3103\n  step 100/312 - loss 0.3183\n  step 150/312 - loss 0.3026\n  step 200/312 - loss 0.2983\n  step 250/312 - loss 0.2956\n  step 300/312 - loss 0.3023\n  train loss 0.3006 | val loss 0.1585 | val acc 0.9530 | 346.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9530)\nLR end of epoch: 3.5308270205019514e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 79/100\n  step 50/312 - loss 0.2952\n  step 100/312 - loss 0.2910\n  step 150/312 - loss 0.2878\n  step 200/312 - loss 0.3038\n  step 250/312 - loss 0.2992\n  step 300/312 - loss 0.2966\n  train loss 0.2959 | val loss 0.1526 | val acc 0.9524 | 345.6s\nLR end of epoch: 3.237182564983431e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 80/100\n  step 50/312 - loss 0.2692\n  step 100/312 - loss 0.2974\n  step 150/312 - loss 0.2888\n  step 200/312 - loss 0.2919\n  step 250/312 - loss 0.2947\n  step 300/312 - loss 0.2956\n  train loss 0.3001 | val loss 0.1561 | val acc 0.9538 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9538)\nLR end of epoch: 2.955195934094537e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 81/100\n  step 50/312 - loss 0.2700\n  step 100/312 - loss 0.2580\n  step 150/312 - loss 0.2784\n  step 200/312 - loss 0.2823\n  step 250/312 - loss 0.2847\n  step 300/312 - loss 0.2827\n  train loss 0.2848 | val loss 0.1537 | val acc 0.9530 | 345.5s\nLR end of epoch: 2.685145414595302e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 82/100\n  step 50/312 - loss 0.2816\n  step 100/312 - loss 0.3081\n  step 150/312 - loss 0.3117\n  step 200/312 - loss 0.3076\n  step 250/312 - loss 0.3101\n  step 300/312 - loss 0.3003\n  train loss 0.2964 | val loss 0.1500 | val acc 0.9536 | 346.1s\nLR end of epoch: 2.4272975137448742e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 83/100\n  step 50/312 - loss 0.2856\n  step 100/312 - loss 0.2785\n  step 150/312 - loss 0.2886\n  step 200/312 - loss 0.2871\n  step 250/312 - loss 0.2881\n  step 300/312 - loss 0.2833\n  train loss 0.2830 | val loss 0.1521 | val acc 0.9538 | 346.2s\nLR end of epoch: 2.181906696291044e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 84/100\n  step 50/312 - loss 0.3408\n  step 100/312 - loss 0.3203\n  step 150/312 - loss 0.3187\n  step 200/312 - loss 0.3108\n  step 250/312 - loss 0.3053\n  step 300/312 - loss 0.3064\n  train loss 0.3050 | val loss 0.1557 | val acc 0.9532 | 346.1s\nLR end of epoch: 1.9492151333442392e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 85/100\n  step 50/312 - loss 0.2787\n  step 100/312 - loss 0.2796\n  step 150/312 - loss 0.2762\n  step 200/312 - loss 0.2801\n  step 250/312 - loss 0.2769\n  step 300/312 - loss 0.2830\n  train loss 0.2796 | val loss 0.1517 | val acc 0.9534 | 346.3s\nLR end of epoch: 1.7294524633839014e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 86/100\n  step 50/312 - loss 0.2972\n  step 100/312 - loss 0.2869\n  step 150/312 - loss 0.2836\n  step 200/312 - loss 0.2841\n  step 250/312 - loss 0.2885\n  step 300/312 - loss 0.2778\n  train loss 0.2800 | val loss 0.1490 | val acc 0.9520 | 345.8s\nLR end of epoch: 1.522835565633007e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 87/100\n  step 50/312 - loss 0.3020\n  step 100/312 - loss 0.2950\n  step 150/312 - loss 0.2916\n  step 200/312 - loss 0.2859\n  step 250/312 - loss 0.2792\n  step 300/312 - loss 0.2854\n  train loss 0.2839 | val loss 0.1536 | val acc 0.9544 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9544)\nLR end of epoch: 1.3295683460244817e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 88/100\n  step 50/312 - loss 0.3231\n  step 100/312 - loss 0.3176\n  step 150/312 - loss 0.2962\n  step 200/312 - loss 0.3028\n  step 250/312 - loss 0.3031\n  step 300/312 - loss 0.2962\n  train loss 0.2988 | val loss 0.1532 | val acc 0.9532 | 346.3s\nLR end of epoch: 1.1498415359706406e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 89/100\n  step 50/312 - loss 0.3154\n  step 100/312 - loss 0.3090\n  step 150/312 - loss 0.3271\n  step 200/312 - loss 0.3192\n  step 250/312 - loss 0.3172\n  step 300/312 - loss 0.3137\n  train loss 0.3149 | val loss 0.1524 | val acc 0.9526 | 345.7s\nLR end of epoch: 9.838325041343294e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 90/100\n  step 50/312 - loss 0.3012\n  step 100/312 - loss 0.2869\n  step 150/312 - loss 0.2900\n  step 200/312 - loss 0.2895\n  step 250/312 - loss 0.2837\n  step 300/312 - loss 0.2805\n  train loss 0.2780 | val loss 0.1481 | val acc 0.9536 | 346.1s\nLR end of epoch: 8.317050813874547e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 91/100\n  step 50/312 - loss 0.2750\n  step 100/312 - loss 0.2919\n  step 150/312 - loss 0.2765\n  step 200/312 - loss 0.2806\n  step 250/312 - loss 0.2906\n  step 300/312 - loss 0.2916\n  train loss 0.2902 | val loss 0.1508 | val acc 0.9534 | 346.2s\nLR end of epoch: 6.936093991297027e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 92/100\n  step 50/312 - loss 0.2897\n  step 100/312 - loss 0.3018\n  step 150/312 - loss 0.3140\n  step 200/312 - loss 0.3053\n  step 250/312 - loss 0.3037\n  step 300/312 - loss 0.3068\n  train loss 0.3064 | val loss 0.1524 | val acc 0.9536 | 346.0s\nLR end of epoch: 5.696817411269653e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 93/100\n  step 50/312 - loss 0.3389\n  step 100/312 - loss 0.2897\n  step 150/312 - loss 0.2925\n  step 200/312 - loss 0.2880\n  step 250/312 - loss 0.2974\n  step 300/312 - loss 0.2977\n  train loss 0.3011 | val loss 0.1525 | val acc 0.9538 | 346.1s\nLR end of epoch: 4.600444090157274e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 94/100\n  step 50/312 - loss 0.3247\n  step 100/312 - loss 0.3255\n  step 150/312 - loss 0.3155\n  step 200/312 - loss 0.3065\n  step 250/312 - loss 0.3042\n  step 300/312 - loss 0.3001\n  train loss 0.2994 | val loss 0.1508 | val acc 0.9544 | 346.6s\nLR end of epoch: 3.648056016061052e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 95/100\n  step 50/312 - loss 0.3265\n  step 100/312 - loss 0.3037\n  step 150/312 - loss 0.2926\n  step 200/312 - loss 0.2972\n  step 250/312 - loss 0.2990\n  step 300/312 - loss 0.2979\n  train loss 0.3016 | val loss 0.1516 | val acc 0.9532 | 346.6s\nLR end of epoch: 2.8405930810269197e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 96/100\n  step 50/312 - loss 0.3180\n  step 100/312 - loss 0.3181\n  step 150/312 - loss 0.3087\n  step 200/312 - loss 0.3046\n  step 250/312 - loss 0.3070\n  step 300/312 - loss 0.3084\n  train loss 0.3088 | val loss 0.1530 | val acc 0.9528 | 346.0s\nLR end of epoch: 2.178852153485574e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 97/100\n  step 50/312 - loss 0.2861\n  step 100/312 - loss 0.2909\n  step 150/312 - loss 0.2829\n  step 200/312 - loss 0.2819\n  step 250/312 - loss 0.2889\n  step 300/312 - loss 0.2852\n  train loss 0.2822 | val loss 0.1507 | val acc 0.9538 | 346.4s\nLR end of epoch: 1.6634862918395403e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 98/100\n  step 50/312 - loss 0.2318\n  step 100/312 - loss 0.2430\n  step 150/312 - loss 0.2571\n  step 200/312 - loss 0.2727\n  step 250/312 - loss 0.2777\n  step 300/312 - loss 0.2788\n  train loss 0.2817 | val loss 0.1497 | val acc 0.9540 | 346.2s\nLR end of epoch: 1.2950040999734018e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 99/100\n  step 50/312 - loss 0.2710\n  step 100/312 - loss 0.2926\n  step 150/312 - loss 0.2816\n  step 200/312 - loss 0.2707\n  step 250/312 - loss 0.2755\n  step 300/312 - loss 0.2777\n  train loss 0.2798 | val loss 0.1495 | val acc 0.9540 | 345.9s\nLR end of epoch: 1.0737692253231258e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 100/100\n  step 50/312 - loss 0.3048\n  step 100/312 - loss 0.3020\n  step 150/312 - loss 0.3032\n  step 200/312 - loss 0.3006\n  step 250/312 - loss 0.2975\n  step 300/312 - loss 0.2954\n  train loss 0.2955 | val loss 0.1505 | val acc 0.9540 | 346.4s\nLR end of epoch: 1e-06\n\nBest val acc: 0.9544\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"Device: cuda\nTrain images: 20000 | Val images: 5000\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 1/100\n  step 50/312 - loss 0.7011 - acc 0.5041\n  step 100/312 - loss 0.7029 - acc 0.4983\n  step 150/312 - loss 0.7053 - acc 0.5018\n  step 200/312 - loss 0.7042 - acc 0.5081\n  step 250/312 - loss 0.7020 - acc 0.5134\n  step 300/312 - loss 0.7004 - acc 0.5193\n  train loss 0.6998 | train acc 0.5208 | val loss 0.6710 | val acc 0.5774 | 345.8s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.5774)\nLR end of epoch: 6.079999999999998e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 2/100\n  step 50/312 - loss 0.6814 - acc 0.5816\n  step 100/312 - loss 0.6852 - acc 0.5708\n  step 150/312 - loss 0.6852 - acc 0.5674\n  step 200/312 - loss 0.6821 - acc 0.5730\n  step 250/312 - loss 0.6798 - acc 0.5760\n  step 300/312 - loss 0.6773 - acc 0.5790\n  train loss 0.6768 | train acc 0.5793 | val loss 0.6516 | val acc 0.6018 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6018)\nLR end of epoch: 0.00012059999999999996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 3/100\n  step 50/312 - loss 0.6725 - acc 0.5928\n  step 100/312 - loss 0.6680 - acc 0.5945\n  step 150/312 - loss 0.6667 - acc 0.5986\n  step 200/312 - loss 0.6677 - acc 0.5956\n  step 250/312 - loss 0.6643 - acc 0.6018\n  step 300/312 - loss 0.6607 - acc 0.6066\n  train loss 0.6596 | train acc 0.6080 | val loss 0.6312 | val acc 0.6298 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6298)\nLR end of epoch: 0.00018039999999999997\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 4/100\n  step 50/312 - loss 0.6389 - acc 0.6409\n  step 100/312 - loss 0.6392 - acc 0.6427\n  step 150/312 - loss 0.6386 - acc 0.6405\n  step 200/312 - loss 0.6356 - acc 0.6449\n  step 250/312 - loss 0.6303 - acc 0.6519\n  step 300/312 - loss 0.6288 - acc 0.6550\n  train loss 0.6292 | train acc 0.6543 | val loss 0.6126 | val acc 0.6548 | 345.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.6548)\nLR end of epoch: 0.00024019999999999993\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 5/100\n  step 50/312 - loss 0.6087 - acc 0.6778\n  step 100/312 - loss 0.6098 - acc 0.6763\n  step 150/312 - loss 0.6079 - acc 0.6773\n  step 200/312 - loss 0.6060 - acc 0.6791\n  step 250/312 - loss 0.6061 - acc 0.6789\n  step 300/312 - loss 0.6038 - acc 0.6821\n  train loss 0.6034 | train acc 0.6830 | val loss 0.5581 | val acc 0.7044 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7044)\nLR end of epoch: 0.00029815940691897306\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 6/100\n  step 50/312 - loss 0.6294\n  step 100/312 - loss 0.6345\n  step 150/312 - loss 0.6253\n  step 200/312 - loss 0.6236\n  step 250/312 - loss 0.6223\n  step 300/312 - loss 0.6209\n  train loss 0.6198 | val loss 0.5408 | val acc 0.7200 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7200)\nLR end of epoch: 0.00029735194398393893\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 7/100\n  step 50/312 - loss 0.6175\n  step 100/312 - loss 0.6143\n  step 150/312 - loss 0.6106\n  step 200/312 - loss 0.6100\n  step 250/312 - loss 0.6095\n  step 300/312 - loss 0.6091\n  train loss 0.6091 | val loss 0.5486 | val acc 0.7290 | 345.4s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7290)\nLR end of epoch: 0.0002963995559098427\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 8/100\n  step 50/312 - loss 0.6070\n  step 100/312 - loss 0.6024\n  step 150/312 - loss 0.6050\n  step 200/312 - loss 0.6006\n  step 250/312 - loss 0.5999\n  step 300/312 - loss 0.5997\n  train loss 0.5989 | val loss 0.5187 | val acc 0.7398 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7398)\nLR end of epoch: 0.00029530318258873034\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 9/100\n  step 50/312 - loss 0.5810\n  step 100/312 - loss 0.5839\n  step 150/312 - loss 0.5810\n  step 200/312 - loss 0.5853\n  step 250/312 - loss 0.5879\n  step 300/312 - loss 0.5872\n  train loss 0.5876 | val loss 0.4951 | val acc 0.7626 | 345.4s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7626)\nLR end of epoch: 0.00029406390600870296\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 10/100\n  step 50/312 - loss 0.5791\n  step 100/312 - loss 0.5770\n  step 150/312 - loss 0.5786\n  step 200/312 - loss 0.5746\n  step 250/312 - loss 0.5741\n  step 300/312 - loss 0.5751\n  train loss 0.5750 | val loss 0.4838 | val acc 0.7764 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7764)\nLR end of epoch: 0.0002926829491861254\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 11/100\n  step 50/312 - loss 0.5487\n  step 100/312 - loss 0.5603\n  step 150/312 - loss 0.5678\n  step 200/312 - loss 0.5675\n  step 250/312 - loss 0.5635\n  step 300/312 - loss 0.5604\n  train loss 0.5610 | val loss 0.4754 | val acc 0.7814 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7814)\nLR end of epoch: 0.0002911616749586567\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 12/100\n  step 50/312 - loss 0.5760\n  step 100/312 - loss 0.5639\n  step 150/312 - loss 0.5616\n  step 200/312 - loss 0.5612\n  step 250/312 - loss 0.5612\n  step 300/312 - loss 0.5618\n  train loss 0.5622 | val loss 0.4590 | val acc 0.7882 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.7882)\nLR end of epoch: 0.00028950158464029354\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 13/100\n  step 50/312 - loss 0.5363\n  step 100/312 - loss 0.5427\n  step 150/312 - loss 0.5505\n  step 200/312 - loss 0.5527\n  step 250/312 - loss 0.5508\n  step 300/312 - loss 0.5481\n  train loss 0.5491 | val loss 0.4381 | val acc 0.8144 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8144)\nLR end of epoch: 0.00028770431653975514\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 14/100\n  step 50/312 - loss 0.5292\n  step 100/312 - loss 0.5346\n  step 150/312 - loss 0.5318\n  step 200/312 - loss 0.5318\n  step 250/312 - loss 0.5329\n  step 300/312 - loss 0.5328\n  train loss 0.5307 | val loss 0.4054 | val acc 0.8142 | 345.9s\nLR end of epoch: 0.0002857716443436699\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 15/100\n  step 50/312 - loss 0.5251\n  step 100/312 - loss 0.5260\n  step 150/312 - loss 0.5283\n  step 200/312 - loss 0.5286\n  step 250/312 - loss 0.5295\n  step 300/312 - loss 0.5320\n  train loss 0.5318 | val loss 0.3795 | val acc 0.8394 | 345.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8394)\nLR end of epoch: 0.00028370547536616097\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 16/100\n  step 50/312 - loss 0.5167\n  step 100/312 - loss 0.5213\n  step 150/312 - loss 0.5203\n  step 200/312 - loss 0.5147\n  step 250/312 - loss 0.5155\n  step 300/312 - loss 0.5167\n  train loss 0.5166 | val loss 0.3836 | val acc 0.8386 | 346.0s\nLR end of epoch: 0.00028150784866655756\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 17/100\n  step 50/312 - loss 0.5131\n  step 100/312 - loss 0.5249\n  step 150/312 - loss 0.5148\n  step 200/312 - loss 0.5096\n  step 250/312 - loss 0.5063\n  step 300/312 - loss 0.5065\n  train loss 0.5066 | val loss 0.3782 | val acc 0.8368 | 345.5s\nLR end of epoch: 0.00027918093303708956\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 18/100\n  step 50/312 - loss 0.4860\n  step 100/312 - loss 0.4884\n  step 150/312 - loss 0.4869\n  step 200/312 - loss 0.4857\n  step 250/312 - loss 0.4880\n  step 300/312 - loss 0.4903\n  train loss 0.4905 | val loss 0.3670 | val acc 0.8464 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8464)\nLR end of epoch: 0.00027672702486255125\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 19/100\n  step 50/312 - loss 0.5028\n  step 100/312 - loss 0.5082\n  step 150/312 - loss 0.5009\n  step 200/312 - loss 0.4999\n  step 250/312 - loss 0.4960\n  step 300/312 - loss 0.4937\n  train loss 0.4935 | val loss 0.3492 | val acc 0.8524 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8524)\nLR end of epoch: 0.00027414854585404696\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 20/100\n  step 50/312 - loss 0.5009\n  step 100/312 - loss 0.5007\n  step 150/312 - loss 0.4956\n  step 200/312 - loss 0.4918\n  step 250/312 - loss 0.4890\n  step 300/312 - loss 0.4819\n  train loss 0.4807 | val loss 0.3250 | val acc 0.8588 | 346.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8588)\nLR end of epoch: 0.0002714480406590546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 21/100\n  step 50/312 - loss 0.4629\n  step 100/312 - loss 0.4682\n  step 150/312 - loss 0.4720\n  step 200/312 - loss 0.4771\n  step 250/312 - loss 0.4771\n  step 300/312 - loss 0.4728\n  train loss 0.4729 | val loss 0.3101 | val acc 0.8762 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8762)\nLR end of epoch: 0.0002686281743501657\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 22/100\n  step 50/312 - loss 0.4863\n  step 100/312 - loss 0.4869\n  step 150/312 - loss 0.4784\n  step 200/312 - loss 0.4734\n  step 250/312 - loss 0.4692\n  step 300/312 - loss 0.4669\n  train loss 0.4659 | val loss 0.2996 | val acc 0.8782 | 346.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8782)\nLR end of epoch: 0.00026569172979498044\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 23/100\n  step 50/312 - loss 0.4712\n  step 100/312 - loss 0.4584\n  step 150/312 - loss 0.4620\n  step 200/312 - loss 0.4650\n  step 250/312 - loss 0.4721\n  step 300/312 - loss 0.4731\n  train loss 0.4724 | val loss 0.3156 | val acc 0.8782 | 346.0s\nLR end of epoch: 0.00026264160490975367\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 24/100\n  step 50/312 - loss 0.4277\n  step 100/312 - loss 0.4312\n  step 150/312 - loss 0.4425\n  step 200/312 - loss 0.4329\n  step 250/312 - loss 0.4338\n  step 300/312 - loss 0.4346\n  train loss 0.4310 | val loss 0.2743 | val acc 0.8844 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8844)\nLR end of epoch: 0.00025948080979950103\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 25/100\n  step 50/312 - loss 0.4628\n  step 100/312 - loss 0.4471\n  step 150/312 - loss 0.4488\n  step 200/312 - loss 0.4456\n  step 250/312 - loss 0.4498\n  step 300/312 - loss 0.4517\n  train loss 0.4529 | val loss 0.2885 | val acc 0.8874 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8874)\nLR end of epoch: 0.00025621246378738883\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 26/100\n  step 50/312 - loss 0.4264\n  step 100/312 - loss 0.4309\n  step 150/312 - loss 0.4430\n  step 200/312 - loss 0.4359\n  step 250/312 - loss 0.4368\n  step 300/312 - loss 0.4327\n  train loss 0.4320 | val loss 0.2716 | val acc 0.8948 | 346.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.8948)\nLR end of epoch: 0.00025283979233633894\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 27/100\n  step 50/312 - loss 0.4492\n  step 100/312 - loss 0.4252\n  step 150/312 - loss 0.4238\n  step 200/312 - loss 0.4232\n  step 250/312 - loss 0.4268\n  step 300/312 - loss 0.4282\n  train loss 0.4286 | val loss 0.2910 | val acc 0.8888 | 345.3s\nLR end of epoch: 0.0002493661238658859\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 28/100\n  step 50/312 - loss 0.4045\n  step 100/312 - loss 0.4247\n  step 150/312 - loss 0.4139\n  step 200/312 - loss 0.4102\n  step 250/312 - loss 0.4087\n  step 300/312 - loss 0.4121\n  train loss 0.4099 | val loss 0.2486 | val acc 0.9096 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9096)\nLR end of epoch: 0.0002457948864674291\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 29/100\n  step 50/312 - loss 0.4183\n  step 100/312 - loss 0.4148\n  step 150/312 - loss 0.4164\n  step 200/312 - loss 0.4174\n  step 250/312 - loss 0.4206\n  step 300/312 - loss 0.4255\n  train loss 0.4241 | val loss 0.2440 | val acc 0.9076 | 346.2s\nLR end of epoch: 0.00024212960452111996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 30/100\n  step 50/312 - loss 0.3939\n  step 100/312 - loss 0.4042\n  step 150/312 - loss 0.4014\n  step 200/312 - loss 0.4067\n  step 250/312 - loss 0.4096\n  step 300/312 - loss 0.4103\n  train loss 0.4106 | val loss 0.2395 | val acc 0.9158 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9158)\nLR end of epoch: 0.0002383738952177247\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 31/100\n  step 50/312 - loss 0.4266\n  step 100/312 - loss 0.4132\n  step 150/312 - loss 0.4040\n  step 200/312 - loss 0.4067\n  step 250/312 - loss 0.4080\n  step 300/312 - loss 0.4072\n  train loss 0.4081 | val loss 0.2431 | val acc 0.9118 | 345.9s\nLR end of epoch: 0.00023453146498889348\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 32/100\n  step 50/312 - loss 0.3986\n  step 100/312 - loss 0.4087\n  step 150/312 - loss 0.3906\n  step 200/312 - loss 0.3893\n  step 250/312 - loss 0.3909\n  step 300/312 - loss 0.3918\n  train loss 0.3915 | val loss 0.2279 | val acc 0.9198 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9198)\nLR end of epoch: 0.00023060610584935996\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 33/100\n  step 50/312 - loss 0.3726\n  step 100/312 - loss 0.4144\n  step 150/312 - loss 0.3979\n  step 200/312 - loss 0.3877\n  step 250/312 - loss 0.3907\n  step 300/312 - loss 0.3938\n  train loss 0.3924 | val loss 0.2069 | val acc 0.9214 | 345.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9214)\nLR end of epoch: 0.00022660169165468046\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 34/100\n  step 50/312 - loss 0.3742\n  step 100/312 - loss 0.3914\n  step 150/312 - loss 0.3948\n  step 200/312 - loss 0.3935\n  step 250/312 - loss 0.3934\n  step 300/312 - loss 0.3953\n  train loss 0.3939 | val loss 0.2108 | val acc 0.9166 | 345.4s\nLR end of epoch: 0.00022252217427820638\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 35/100\n  step 50/312 - loss 0.3772\n  step 100/312 - loss 0.3718\n  step 150/312 - loss 0.3777\n  step 200/312 - loss 0.3774\n  step 250/312 - loss 0.3722\n  step 300/312 - loss 0.3748\n  train loss 0.3734 | val loss 0.2064 | val acc 0.9266 | 345.9s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9266)\nLR end of epoch: 0.0002183715797110622\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 36/100\n  step 50/312 - loss 0.3523\n  step 100/312 - loss 0.3615\n  step 150/312 - loss 0.3721\n  step 200/312 - loss 0.3758\n  step 250/312 - loss 0.3755\n  step 300/312 - loss 0.3718\n  train loss 0.3710 | val loss 0.2158 | val acc 0.9204 | 345.7s\nLR end of epoch: 0.00021415400408897832\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 37/100\n  step 50/312 - loss 0.3398\n  step 100/312 - loss 0.3676\n  step 150/312 - loss 0.3705\n  step 200/312 - loss 0.3724\n  step 250/312 - loss 0.3772\n  step 300/312 - loss 0.3776\n  train loss 0.3783 | val loss 0.2242 | val acc 0.9258 | 345.8s\nLR end of epoch: 0.00020987360964989964\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 38/100\n  step 50/312 - loss 0.3561\n  step 100/312 - loss 0.3710\n  step 150/312 - loss 0.3736\n  step 200/312 - loss 0.3844\n  step 250/312 - loss 0.3754\n  step 300/312 - loss 0.3720\n  train loss 0.3719 | val loss 0.2174 | val acc 0.9258 | 346.2s\nLR end of epoch: 0.0002055346206263593\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 39/100\n  step 50/312 - loss 0.3524\n  step 100/312 - loss 0.3571\n  step 150/312 - loss 0.3668\n  step 200/312 - loss 0.3682\n  step 250/312 - loss 0.3652\n  step 300/312 - loss 0.3570\n  train loss 0.3586 | val loss 0.1923 | val acc 0.9322 | 346.1s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9322)\nLR end of epoch: 0.00020114131907667107\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 40/100\n  step 50/312 - loss 0.3368\n  step 100/312 - loss 0.3618\n  step 150/312 - loss 0.3554\n  step 200/312 - loss 0.3606\n  step 250/312 - loss 0.3628\n  step 300/312 - loss 0.3587\n  train loss 0.3608 | val loss 0.2117 | val acc 0.9296 | 345.8s\nLR end of epoch: 0.0001966980406590546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 41/100\n  step 50/312 - loss 0.3372\n  step 100/312 - loss 0.3483\n  step 150/312 - loss 0.3538\n  step 200/312 - loss 0.3591\n  step 250/312 - loss 0.3559\n  step 300/312 - loss 0.3551\n  train loss 0.3557 | val loss 0.2047 | val acc 0.9304 | 344.8s\nLR end of epoch: 0.00019220917035286475\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 42/100\n  step 50/312 - loss 0.3521\n  step 100/312 - loss 0.3497\n  step 150/312 - loss 0.3422\n  step 200/312 - loss 0.3529\n  step 250/312 - loss 0.3566\n  step 300/312 - loss 0.3580\n  train loss 0.3573 | val loss 0.1884 | val acc 0.9376 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9376)\nLR end of epoch: 0.00018767913813114574\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 43/100\n  step 50/312 - loss 0.3162\n  step 100/312 - loss 0.3412\n  step 150/312 - loss 0.3425\n  step 200/312 - loss 0.3473\n  step 250/312 - loss 0.3447\n  step 300/312 - loss 0.3478\n  train loss 0.3466 | val loss 0.2066 | val acc 0.9280 | 345.5s\nLR end of epoch: 0.00018311241458878307\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 44/100\n  step 50/312 - loss 0.3406\n  step 100/312 - loss 0.3546\n  step 150/312 - loss 0.3547\n  step 200/312 - loss 0.3566\n  step 250/312 - loss 0.3554\n  step 300/312 - loss 0.3509\n  train loss 0.3490 | val loss 0.1781 | val acc 0.9342 | 345.9s\nLR end of epoch: 0.0001785135065305658\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 45/100\n  step 50/312 - loss 0.3345\n  step 100/312 - loss 0.3523\n  step 150/312 - loss 0.3562\n  step 200/312 - loss 0.3607\n  step 250/312 - loss 0.3660\n  step 300/312 - loss 0.3595\n  train loss 0.3605 | val loss 0.2097 | val acc 0.9288 | 345.1s\nLR end of epoch: 0.00017388695252351449\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 46/100\n  step 50/312 - loss 0.3409\n  step 100/312 - loss 0.3257\n  step 150/312 - loss 0.3325\n  step 200/312 - loss 0.3344\n  step 250/312 - loss 0.3467\n  step 300/312 - loss 0.3489\n  train loss 0.3499 | val loss 0.1986 | val acc 0.9376 | 346.2s\nLR end of epoch: 0.00016923731841786345\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 47/100\n  step 50/312 - loss 0.3298\n  step 100/312 - loss 0.3383\n  step 150/312 - loss 0.3474\n  step 200/312 - loss 0.3440\n  step 250/312 - loss 0.3450\n  step 300/312 - loss 0.3499\n  train loss 0.3492 | val loss 0.1799 | val acc 0.9342 | 345.9s\nLR end of epoch: 0.0001645691928411179\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 48/100\n  step 50/312 - loss 0.3051\n  step 100/312 - loss 0.3186\n  step 150/312 - loss 0.3258\n  step 200/312 - loss 0.3345\n  step 250/312 - loss 0.3391\n  step 300/312 - loss 0.3346\n  train loss 0.3328 | val loss 0.1851 | val acc 0.9354 | 346.1s\nLR end of epoch: 0.00015988718266963235\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 49/100\n  step 50/312 - loss 0.2935\n  step 100/312 - loss 0.3184\n  step 150/312 - loss 0.3241\n  step 200/312 - loss 0.3320\n  step 250/312 - loss 0.3374\n  step 300/312 - loss 0.3323\n  train loss 0.3341 | val loss 0.1861 | val acc 0.9394 | 345.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9394)\nLR end of epoch: 0.00015519590848218014\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 50/100\n  step 50/312 - loss 0.3216\n  step 100/312 - loss 0.3340\n  step 150/312 - loss 0.3340\n  step 200/312 - loss 0.3396\n  step 250/312 - loss 0.3392\n  step 300/312 - loss 0.3351\n  train loss 0.3368 | val loss 0.2013 | val acc 0.9342 | 345.7s\nLR end of epoch: 0.00015049999999999997\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 51/100\n  step 50/312 - loss 0.3464\n  step 100/312 - loss 0.3393\n  step 150/312 - loss 0.3353\n  step 200/312 - loss 0.3369\n  step 250/312 - loss 0.3355\n  step 300/312 - loss 0.3487\n  train loss 0.3486 | val loss 0.1921 | val acc 0.9368 | 345.7s\nLR end of epoch: 0.00014580409151781983\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 52/100\n  step 50/312 - loss 0.3946\n  step 100/312 - loss 0.3767\n  step 150/312 - loss 0.3791\n  step 200/312 - loss 0.3730\n  step 250/312 - loss 0.3691\n  step 300/312 - loss 0.3585\n  train loss 0.3580 | val loss 0.1995 | val acc 0.9344 | 346.0s\nLR end of epoch: 0.00014111281733036765\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 53/100\n  step 50/312 - loss 0.3376\n  step 100/312 - loss 0.3472\n  step 150/312 - loss 0.3343\n  step 200/312 - loss 0.3286\n  step 250/312 - loss 0.3286\n  step 300/312 - loss 0.3232\n  train loss 0.3238 | val loss 0.1814 | val acc 0.9366 | 345.4s\nLR end of epoch: 0.00013643080715888212\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 54/100\n  step 50/312 - loss 0.3423\n  step 100/312 - loss 0.3465\n  step 150/312 - loss 0.3207\n  step 200/312 - loss 0.3298\n  step 250/312 - loss 0.3256\n  step 300/312 - loss 0.3259\n  train loss 0.3250 | val loss 0.1777 | val acc 0.9430 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9430)\nLR end of epoch: 0.00013176268158213652\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 55/100\n  step 50/312 - loss 0.3803\n  step 100/312 - loss 0.3491\n  step 150/312 - loss 0.3442\n  step 200/312 - loss 0.3380\n  step 250/312 - loss 0.3354\n  step 300/312 - loss 0.3354\n  train loss 0.3362 | val loss 0.1857 | val acc 0.9420 | 345.5s\nLR end of epoch: 0.00012711304747648546\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 56/100\n  step 50/312 - loss 0.3689\n  step 100/312 - loss 0.3595\n  step 150/312 - loss 0.3529\n  step 200/312 - loss 0.3360\n  step 250/312 - loss 0.3327\n  step 300/312 - loss 0.3267\n  train loss 0.3265 | val loss 0.1713 | val acc 0.9408 | 345.6s\nLR end of epoch: 0.00012248649346943414\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 57/100\n  step 50/312 - loss 0.3069\n  step 100/312 - loss 0.3204\n  step 150/312 - loss 0.3339\n  step 200/312 - loss 0.3427\n  step 250/312 - loss 0.3479\n  step 300/312 - loss 0.3500\n  train loss 0.3473 | val loss 0.1741 | val acc 0.9430 | 345.5s\nLR end of epoch: 0.0001178875854112169\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 58/100\n  step 50/312 - loss 0.3442\n  step 100/312 - loss 0.3329\n  step 150/312 - loss 0.3291\n  step 200/312 - loss 0.3267\n  step 250/312 - loss 0.3251\n  step 300/312 - loss 0.3192\n  train loss 0.3178 | val loss 0.1716 | val acc 0.9424 | 345.6s\nLR end of epoch: 0.00011332086186885422\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 59/100\n  step 50/312 - loss 0.3495\n  step 100/312 - loss 0.3154\n  step 150/312 - loss 0.3198\n  step 200/312 - loss 0.3254\n  step 250/312 - loss 0.3253\n  step 300/312 - loss 0.3232\n  train loss 0.3216 | val loss 0.1793 | val acc 0.9382 | 345.8s\nLR end of epoch: 0.00010879082964713522\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 60/100\n  step 50/312 - loss 0.2933\n  step 100/312 - loss 0.3033\n  step 150/312 - loss 0.3192\n  step 200/312 - loss 0.3285\n  step 250/312 - loss 0.3306\n  step 300/312 - loss 0.3259\n  train loss 0.3279 | val loss 0.1720 | val acc 0.9418 | 346.2s\nLR end of epoch: 0.00010430195934094535\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 61/100\n  step 50/312 - loss 0.2791\n  step 100/312 - loss 0.3080\n  step 150/312 - loss 0.3051\n  step 200/312 - loss 0.3119\n  step 250/312 - loss 0.3117\n  step 300/312 - loss 0.3140\n  train loss 0.3116 | val loss 0.1675 | val acc 0.9454 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9454)\nLR end of epoch: 9.985868092332892e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 62/100\n  step 50/312 - loss 0.2584\n  step 100/312 - loss 0.2874\n  step 150/312 - loss 0.2955\n  step 200/312 - loss 0.3020\n  step 250/312 - loss 0.3090\n  step 300/312 - loss 0.3073\n  train loss 0.3091 | val loss 0.1651 | val acc 0.9454 | 346.1s\nLR end of epoch: 9.546537937364064e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 63/100\n  step 50/312 - loss 0.3186\n  step 100/312 - loss 0.3227\n  step 150/312 - loss 0.3126\n  step 200/312 - loss 0.3338\n  step 250/312 - loss 0.3309\n  step 300/312 - loss 0.3247\n  train loss 0.3293 | val loss 0.1779 | val acc 0.9452 | 345.6s\nLR end of epoch: 9.11263903501003e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 64/100\n  step 50/312 - loss 0.3432\n  step 100/312 - loss 0.3264\n  step 150/312 - loss 0.3136\n  step 200/312 - loss 0.3160\n  step 250/312 - loss 0.3156\n  step 300/312 - loss 0.3141\n  train loss 0.3143 | val loss 0.1579 | val acc 0.9470 | 345.7s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9470)\nLR end of epoch: 8.684599591102167e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 65/100\n  step 50/312 - loss 0.2691\n  step 100/312 - loss 0.3118\n  step 150/312 - loss 0.3015\n  step 200/312 - loss 0.3007\n  step 250/312 - loss 0.2975\n  step 300/312 - loss 0.2968\n  train loss 0.2989 | val loss 0.1727 | val acc 0.9446 | 345.6s\nLR end of epoch: 8.262842028893775e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 66/100\n  step 50/312 - loss 0.3495\n  step 100/312 - loss 0.3441\n  step 150/312 - loss 0.3330\n  step 200/312 - loss 0.3178\n  step 250/312 - loss 0.3184\n  step 300/312 - loss 0.3124\n  train loss 0.3095 | val loss 0.1570 | val acc 0.9442 | 345.5s\nLR end of epoch: 7.84778257217936e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 67/100\n  step 50/312 - loss 0.2986\n  step 100/312 - loss 0.3086\n  step 150/312 - loss 0.3109\n  step 200/312 - loss 0.3121\n  step 250/312 - loss 0.3076\n  step 300/312 - loss 0.3080\n  train loss 0.3119 | val loss 0.1665 | val acc 0.9484 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9484)\nLR end of epoch: 7.439830834531953e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 68/100\n  step 50/312 - loss 0.3663\n  step 100/312 - loss 0.3257\n  step 150/312 - loss 0.3194\n  step 200/312 - loss 0.3085\n  step 250/312 - loss 0.3120\n  step 300/312 - loss 0.3153\n  train loss 0.3144 | val loss 0.1637 | val acc 0.9500 | 346.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9500)\nLR end of epoch: 7.039389415064002e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 69/100\n  step 50/312 - loss 0.3380\n  step 100/312 - loss 0.3121\n  step 150/312 - loss 0.3195\n  step 200/312 - loss 0.3182\n  step 250/312 - loss 0.3133\n  step 300/312 - loss 0.3139\n  train loss 0.3149 | val loss 0.1600 | val acc 0.9472 | 346.0s\nLR end of epoch: 6.646853501110649e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 70/100\n  step 50/312 - loss 0.3114\n  step 100/312 - loss 0.3189\n  step 150/312 - loss 0.3213\n  step 200/312 - loss 0.3238\n  step 250/312 - loss 0.3277\n  step 300/312 - loss 0.3279\n  train loss 0.3224 | val loss 0.1489 | val acc 0.9496 | 345.9s\nLR end of epoch: 6.262610478227527e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 71/100\n  step 50/312 - loss 0.3095\n  step 100/312 - loss 0.2875\n  step 150/312 - loss 0.3044\n  step 200/312 - loss 0.2983\n  step 250/312 - loss 0.3025\n  step 300/312 - loss 0.3010\n  train loss 0.2992 | val loss 0.1550 | val acc 0.9502 | 345.0s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9502)\nLR end of epoch: 5.8870395478880035e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 72/100\n  step 50/312 - loss 0.2792\n  step 100/312 - loss 0.3184\n  step 150/312 - loss 0.3278\n  step 200/312 - loss 0.3318\n  step 250/312 - loss 0.3284\n  step 300/312 - loss 0.3268\n  train loss 0.3297 | val loss 0.1733 | val acc 0.9484 | 345.6s\nLR end of epoch: 5.520511353257087e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 73/100\n  step 50/312 - loss 0.3186\n  step 100/312 - loss 0.3035\n  step 150/312 - loss 0.3140\n  step 200/312 - loss 0.3191\n  step 250/312 - loss 0.3216\n  step 300/312 - loss 0.3190\n  train loss 0.3188 | val loss 0.1660 | val acc 0.9510 | 345.3s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9510)\nLR end of epoch: 5.1633876134114055e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 74/100\n  step 50/312 - loss 0.2983\n  step 100/312 - loss 0.3163\n  step 150/312 - loss 0.3108\n  step 200/312 - loss 0.3166\n  step 250/312 - loss 0.3186\n  step 300/312 - loss 0.3176\n  train loss 0.3184 | val loss 0.1590 | val acc 0.9484 | 346.5s\nLR end of epoch: 4.816020766366103e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 75/100\n  step 50/312 - loss 0.3122\n  step 100/312 - loss 0.2904\n  step 150/312 - loss 0.2958\n  step 200/312 - loss 0.2950\n  step 250/312 - loss 0.3025\n  step 300/312 - loss 0.2970\n  train loss 0.2955 | val loss 0.1466 | val acc 0.9510 | 345.7s\nLR end of epoch: 4.478753621261114e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 76/100\n  step 50/312 - loss 0.3129\n  step 100/312 - loss 0.3099\n  step 150/312 - loss 0.3122\n  step 200/312 - loss 0.3117\n  step 250/312 - loss 0.3001\n  step 300/312 - loss 0.3078\n  train loss 0.3066 | val loss 0.1529 | val acc 0.9512 | 345.9s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9512)\nLR end of epoch: 4.1519190200498946e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 77/100\n  step 50/312 - loss 0.3234\n  step 100/312 - loss 0.2983\n  step 150/312 - loss 0.2982\n  step 200/312 - loss 0.3094\n  step 250/312 - loss 0.3062\n  step 300/312 - loss 0.3017\n  train loss 0.3016 | val loss 0.1512 | val acc 0.9526 | 345.8s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9526)\nLR end of epoch: 3.835839509024628e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 78/100\n  step 50/312 - loss 0.3103\n  step 100/312 - loss 0.3183\n  step 150/312 - loss 0.3026\n  step 200/312 - loss 0.2983\n  step 250/312 - loss 0.2956\n  step 300/312 - loss 0.3023\n  train loss 0.3006 | val loss 0.1585 | val acc 0.9530 | 346.5s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9530)\nLR end of epoch: 3.5308270205019514e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 79/100\n  step 50/312 - loss 0.2952\n  step 100/312 - loss 0.2910\n  step 150/312 - loss 0.2878\n  step 200/312 - loss 0.3038\n  step 250/312 - loss 0.2992\n  step 300/312 - loss 0.2966\n  train loss 0.2959 | val loss 0.1526 | val acc 0.9524 | 345.6s\nLR end of epoch: 3.237182564983431e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 80/100\n  step 50/312 - loss 0.2692\n  step 100/312 - loss 0.2974\n  step 150/312 - loss 0.2888\n  step 200/312 - loss 0.2919\n  step 250/312 - loss 0.2947\n  step 300/312 - loss 0.2956\n  train loss 0.3001 | val loss 0.1561 | val acc 0.9538 | 346.2s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9538)\nLR end of epoch: 2.955195934094537e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 81/100\n  step 50/312 - loss 0.2700\n  step 100/312 - loss 0.2580\n  step 150/312 - loss 0.2784\n  step 200/312 - loss 0.2823\n  step 250/312 - loss 0.2847\n  step 300/312 - loss 0.2827\n  train loss 0.2848 | val loss 0.1537 | val acc 0.9530 | 345.5s\nLR end of epoch: 2.685145414595302e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 82/100\n  step 50/312 - loss 0.2816\n  step 100/312 - loss 0.3081\n  step 150/312 - loss 0.3117\n  step 200/312 - loss 0.3076\n  step 250/312 - loss 0.3101\n  step 300/312 - loss 0.3003\n  train loss 0.2964 | val loss 0.1500 | val acc 0.9536 | 346.1s\nLR end of epoch: 2.4272975137448742e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 83/100\n  step 50/312 - loss 0.2856\n  step 100/312 - loss 0.2785\n  step 150/312 - loss 0.2886\n  step 200/312 - loss 0.2871\n  step 250/312 - loss 0.2881\n  step 300/312 - loss 0.2833\n  train loss 0.2830 | val loss 0.1521 | val acc 0.9538 | 346.2s\nLR end of epoch: 2.181906696291044e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 84/100\n  step 50/312 - loss 0.3408\n  step 100/312 - loss 0.3203\n  step 150/312 - loss 0.3187\n  step 200/312 - loss 0.3108\n  step 250/312 - loss 0.3053\n  step 300/312 - loss 0.3064\n  train loss 0.3050 | val loss 0.1557 | val acc 0.9532 | 346.1s\nLR end of epoch: 1.9492151333442392e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 85/100\n  step 50/312 - loss 0.2787\n  step 100/312 - loss 0.2796\n  step 150/312 - loss 0.2762\n  step 200/312 - loss 0.2801\n  step 250/312 - loss 0.2769\n  step 300/312 - loss 0.2830\n  train loss 0.2796 | val loss 0.1517 | val acc 0.9534 | 346.3s\nLR end of epoch: 1.7294524633839014e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 86/100\n  step 50/312 - loss 0.2972\n  step 100/312 - loss 0.2869\n  step 150/312 - loss 0.2836\n  step 200/312 - loss 0.2841\n  step 250/312 - loss 0.2885\n  step 300/312 - loss 0.2778\n  train loss 0.2800 | val loss 0.1490 | val acc 0.9520 | 345.8s\nLR end of epoch: 1.522835565633007e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 87/100\n  step 50/312 - loss 0.3020\n  step 100/312 - loss 0.2950\n  step 150/312 - loss 0.2916\n  step 200/312 - loss 0.2859\n  step 250/312 - loss 0.2792\n  step 300/312 - loss 0.2854\n  train loss 0.2839 | val loss 0.1536 | val acc 0.9544 | 345.6s\n  Saved best EMA checkpoint to /kaggle/working/best_ema.pt (acc=0.9544)\nLR end of epoch: 1.3295683460244817e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 88/100\n  step 50/312 - loss 0.3231\n  step 100/312 - loss 0.3176\n  step 150/312 - loss 0.2962\n  step 200/312 - loss 0.3028\n  step 250/312 - loss 0.3031\n  step 300/312 - loss 0.2962\n  train loss 0.2988 | val loss 0.1532 | val acc 0.9532 | 346.3s\nLR end of epoch: 1.1498415359706406e-05\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 89/100\n  step 50/312 - loss 0.3154\n  step 100/312 - loss 0.3090\n  step 150/312 - loss 0.3271\n  step 200/312 - loss 0.3192\n  step 250/312 - loss 0.3172\n  step 300/312 - loss 0.3137\n  train loss 0.3149 | val loss 0.1524 | val acc 0.9526 | 345.7s\nLR end of epoch: 9.838325041343294e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 90/100\n  step 50/312 - loss 0.3012\n  step 100/312 - loss 0.2869\n  step 150/312 - loss 0.2900\n  step 200/312 - loss 0.2895\n  step 250/312 - loss 0.2837\n  step 300/312 - loss 0.2805\n  train loss 0.2780 | val loss 0.1481 | val acc 0.9536 | 346.1s\nLR end of epoch: 8.317050813874547e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 91/100\n  step 50/312 - loss 0.2750\n  step 100/312 - loss 0.2919\n  step 150/312 - loss 0.2765\n  step 200/312 - loss 0.2806\n  step 250/312 - loss 0.2906\n  step 300/312 - loss 0.2916\n  train loss 0.2902 | val loss 0.1508 | val acc 0.9534 | 346.2s\nLR end of epoch: 6.936093991297027e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 92/100\n  step 50/312 - loss 0.2897\n  step 100/312 - loss 0.3018\n  step 150/312 - loss 0.3140\n  step 200/312 - loss 0.3053\n  step 250/312 - loss 0.3037\n  step 300/312 - loss 0.3068\n  train loss 0.3064 | val loss 0.1524 | val acc 0.9536 | 346.0s\nLR end of epoch: 5.696817411269653e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 93/100\n  step 50/312 - loss 0.3389\n  step 100/312 - loss 0.2897\n  step 150/312 - loss 0.2925\n  step 200/312 - loss 0.2880\n  step 250/312 - loss 0.2974\n  step 300/312 - loss 0.2977\n  train loss 0.3011 | val loss 0.1525 | val acc 0.9538 | 346.1s\nLR end of epoch: 4.600444090157274e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 94/100\n  step 50/312 - loss 0.3247\n  step 100/312 - loss 0.3255\n  step 150/312 - loss 0.3155\n  step 200/312 - loss 0.3065\n  step 250/312 - loss 0.3042\n  step 300/312 - loss 0.3001\n  train loss 0.2994 | val loss 0.1508 | val acc 0.9544 | 346.6s\nLR end of epoch: 3.648056016061052e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 95/100\n  step 50/312 - loss 0.3265\n  step 100/312 - loss 0.3037\n  step 150/312 - loss 0.2926\n  step 200/312 - loss 0.2972\n  step 250/312 - loss 0.2990\n  step 300/312 - loss 0.2979\n  train loss 0.3016 | val loss 0.1516 | val acc 0.9532 | 346.6s\nLR end of epoch: 2.8405930810269197e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 96/100\n  step 50/312 - loss 0.3180\n  step 100/312 - loss 0.3181\n  step 150/312 - loss 0.3087\n  step 200/312 - loss 0.3046\n  step 250/312 - loss 0.3070\n  step 300/312 - loss 0.3084\n  train loss 0.3088 | val loss 0.1530 | val acc 0.9528 | 346.0s\nLR end of epoch: 2.178852153485574e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 97/100\n  step 50/312 - loss 0.2861\n  step 100/312 - loss 0.2909\n  step 150/312 - loss 0.2829\n  step 200/312 - loss 0.2819\n  step 250/312 - loss 0.2889\n  step 300/312 - loss 0.2852\n  train loss 0.2822 | val loss 0.1507 | val acc 0.9538 | 346.4s\nLR end of epoch: 1.6634862918395403e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 98/100\n  step 50/312 - loss 0.2318\n  step 100/312 - loss 0.2430\n  step 150/312 - loss 0.2571\n  step 200/312 - loss 0.2727\n  step 250/312 - loss 0.2777\n  step 300/312 - loss 0.2788\n  train loss 0.2817 | val loss 0.1497 | val acc 0.9540 | 346.2s\nLR end of epoch: 1.2950040999734018e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 99/100\n  step 50/312 - loss 0.2710\n  step 100/312 - loss 0.2926\n  step 150/312 - loss 0.2816\n  step 200/312 - loss 0.2707\n  step 250/312 - loss 0.2755\n  step 300/312 - loss 0.2777\n  train loss 0.2798 | val loss 0.1495 | val acc 0.9540 | 345.9s\nLR end of epoch: 1.0737692253231258e-06\n\n{'model': 'convnext_small', 'lr': 0.0003, 'batch': 64, 'drop_path': 0.05, 'label_smoothing': 0.05, 'mixup_alpha': 0.1, 'cutmix_alpha': 0.2}\nEpoch 100/100\n  step 50/312 - loss 0.3048\n  step 100/312 - loss 0.3020\n  step 150/312 - loss 0.3032\n  step 200/312 - loss 0.3006\n  step 250/312 - loss 0.2975\n  step 300/312 - loss 0.2954\n  train loss 0.2955 | val loss 0.1505 | val acc 0.9540 | 346.4s\nLR end of epoch: 1e-06\n\nBest val acc: 0.9544","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# =========================\n# Cell 10 - Load best checkpoint (optional)\n# =========================\nckpt = torch.load(cfg.save_path, map_location=\"cpu\")\nprint(\"Best acc in file:\", ckpt[\"best_acc\"])\nprint(\"Model:\", ckpt[\"model_name\"])\n\nbest_model = timm.create_model(\n    ckpt[\"model_name\"],\n    pretrained=False,\n    num_classes=cfg.num_classes\n)\nbest_model.load_state_dict(ckpt[\"model_state\"])\nbest_model = best_model.to(cfg.device)\n\nval_loss, val_acc = evaluate(best_model)\nprint(\"Recomputed val acc:\", val_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T06:02:25.678559Z","iopub.execute_input":"2025-12-07T06:02:25.679111Z","iopub.status.idle":"2025-12-07T06:02:49.863093Z","shell.execute_reply.started":"2025-12-07T06:02:25.679081Z","shell.execute_reply":"2025-12-07T06:02:49.862265Z"}},"outputs":[{"name":"stdout","text":"Best acc in file: 0.9544\nModel: convnext_small\nRecomputed val acc: 0.9544\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"Best acc in file: 0.9544\nModel: convnext_small\nRecomputed val acc: 0.9544","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 11 - Quick knobs to try\n# =========================\n# 1) If you want to test ConvNeXt-Base from scratch:\n# cfg.model_name = \"convnext_base\"\n# cfg.batch_size = 32  # maybe 24 if OOM\n#\n# 2) If val stalls:\n# cfg.mixup_alpha = 0.1\n# cfg.cutmix_alpha = 0.2\n# cfg.drop_path_rate = 0.05\n# cfg.lr = 2e-4\n#\n# Then re-run Cells 6-9.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T06:02:49.864707Z","iopub.execute_input":"2025-12-07T06:02:49.865228Z","iopub.status.idle":"2025-12-07T06:02:49.868874Z","shell.execute_reply.started":"2025-12-07T06:02:49.865201Z","shell.execute_reply":"2025-12-07T06:02:49.868239Z"}},"outputs":[],"execution_count":51}]}